---
title: Residual Flow
date: 2022-10-28 12:34
category: [Paper Review]
tags: [paper review, NeurIPS]
math: true
published: false
img_path: "/assets/img/posts/2022-10-28-RF"
---

# 작성중...

# Residual Flows for Invertible Generative Modeling 
## Unbiased Log Density Estimation for Maximum Likelihood Estimation

mean-bro

## Memory-Efficient Backpropagation
메모리는 매우 부족한 자원입니다. 편향되지 않은 추정기의 큰 샘플은 메모리 부족으로 인한 학습 중단을 야기할 수 있습니다. 이러한 문제를 해결하기 위해 저자는 두 가지 방법을 제시합니다.

멱급수를 통한 직접적인 미분에 의한 파라미터 $\theta$에 대한 기울기는 아래와 같이 표현됩니다.

$$ \frac{}{}\log\det(I+J_g({x,\theta}))=\mathbb{E}_{n,v}\Bigg\lbrack\sum^n_{k=1}\frac{(-1)^{k+1}}{k}\frac{\partial v^T(J_g(x,\theta)^k)v}{\partial\theta}\Bigg\rbrack$$

불행히도, 이 추정기는 각 항에 대한 $\partial/{\partial\theta}$이 필요하기 때문에 모든 항을 메모리에 저장해야 됩니다. 공간복잡도는 $\mathcal{O}(n \sdot m)$이며 여기서 $n$은 계산되어야 하는 항의 수, $m$은 전체 네트워크에서 residual block의 수를 의미합니다. 이것은 훈련시 매우 많은 메모리를 요구하는 것이며, 가끔씩 매우 큰 랜덤 샘플에 의해 메모리 부족이 발생할 수 있습니다.

### Neumann gradient series
저자들은 노이만 급수로부터 유도된 멱급수로 기울기를 표현합니다. 러시안 룰렛과 대각합 추정기를 적용하면, 아래와 같은 이론을 얻을 수 있습니다.

**Therorem 2**

$$\textit{Let} \space \mathrm{Lip}(g) \lt \textit{ and } N \textit{be a random variable with support over positive integers.}$$

$$\frac{\partial}{\partial\theta}\log\det(I+J_g(x,\theta))=\mathbb{E}_{n,v}\Bigg\lbrack\Bigg\lparen\sum^n_{k=0}\frac{(-1)^k}{\mathbb{P}(N\ge k)}v^TJ(x,\theta)^k\Bigg\rparen\frac{\partial(J_g(x,\theta))}{\partial\theta}v\Bigg\rbrack, \\ \space where \space n \sim P(N) \space and \space v \sim \mathcal{N}(0,I)$$

여기서 멱급수는 더 이상 미분할 필요가 없어지므로, 메모리 사용량을 줄일 수 있습니다. 

### Backward-in-forward: early computation of gradients
저자들은 메모리 사용량을 줄이기위해 포워드 평가동안 역전파를 부분적으로 수행할 수 있도록 하였습니다. $\log\det(I+J_g(x,\theta))$이 스칼라량인 점을 활용하여, 목적함수 $\mathcal{L}$의 편미분은 아래 식과 같이 표현될 수 있습니다.

$$ \frac{\partial\mathcal{L}}{\partial\theta}=\underbrace{\frac{\partial\mathcal{L}}{\partial\log\det(I+J_g(x,\theta))}}_{\text{scalar}}\underbrace {\frac{\partial\log\det(I+J_g(x,\theta))}{\partial\theta}}_{\text{vector}}$$

모든 residual block에 대해, forward pass를 할 때 $\partial \log\det(I+J_g(x,\theta))/\partial\theta$를 계산하고, computation graph에 대해 메모리를 해제합니다. 그 다음 주요 역전파에서는 $\partial\mathcal{L}/\log\det(I+J_g(x,\theta))$를 곱합니다. 이 방법은 $m$을 $\mathcal{O}(1)$로 줄입니다.

이 두가지 방법은 $\log \det$ 항을 통해 역전파시 메모리 사용량을 줄이며, $\log(p(f(x))$의 path-wise 도함수를 계산하는 것은 여전히 단일 평가 때와 같은 양의 메모리를 요구합니다.

![Figure 3](figure_3.png){: .align-center}
*Figure 3*

Figure 3을 통해 일반적인 역전파에 비해 메모리 사용량이 크게 감소한 것을 확인할 수 있습니다. 이는 큰 신경망을 다룰 수 있다는 의미이기도 합니다.

## Avoiding Derivative Saturation with the LipSwish Activation Function

log density는 Jacobian 행렬 $J_g$을 통한 일계도함수에 따라 달라지기 때문에 훈련을 위한 기울기는 이계도함수에 따라 달라집니다.
포화된 활성 함수의 현상과 유사하게 Lipschitz-constrained 활성함수는 도함수 포화 문제를 가지고 있습니다. 예를 들어, ELU 활성함수는 $\mathrm{ELU}^\prime(z)=1$일 때 가장 높은 Lipschitz 상수를 달성합니다. 하지만, 이는 이계도함수가 매우 큰 영역에서 정확히 0이 되며 큰 Lipschitz 상수와 기울기 소실이 되지 않도록 하는 것 사이에는 trade-off 관계가 있음을 암시합니다.

그러므로, 활성함수 $\phi(z)$는 아래의 두가지 특성을 만족해야 합니다.

1. 일계도함수는 모든 $z$에 대해 $\vert\phi^\prime(z)\vert\le1$로 제한되어야 합니다.

2. 이계도함수는 $\vert\phi^\prime(z)\vert$가 1에 가까워질 때 소실되어서는 안됩니다. 

![Figure 4](figure_4.png){: .align-center}
*Figure 4: 일반적인 smooth Lipschitz 활성 함수 $\phi$는 $\phi^\prime$이 최대가 됐을 때 $\phi^{\prime\prime}$이 소실되는 문제가 생깁니다. \\
LipSwish은 $\phi^\prime$가 0으로 가까워져도 $\phi^{\prime\prime}$가 소실되지 않습니다.*

많은 활성함수들이 1번 조건은 만족하지만 2번 조건은 만족하지 못합니다. Figure 4는 Lipschitz의 영역에서 softplus와 ELU 함수가 포화될 때, 이계도함수가 0으로 가는 것을 볼 수 있습니다. 이러한 점은 학습 중에 기울기 소실 문제를 야기할 수 있습니다.

저자들은 2번 조건을 만족하는 *smooth and non-monotonic* 함수인 Swish를 발견하였습니다. 그러나, Swish는 1번 조건을 만족하지 못합니다. 그래서, 스케일링을 통해 1번 조건을 만족하도록 간단히 수정하였습니다.

$$\mathrm{LipSwish}(z):=\mathrm{Swish}(z)/1.1=z\sdot\sigma(\beta z)/1.1$$

실제 실험에서는 $\beta$는 softplus 함수를 통해 양수가 되도록 제한합니다. Figure 4를 통해 Lipschitz의 영역에서 non-monotonic 특성에 때문에 LipSwish가 포화되지않음을 확인할 수 있습니다.

# Experiments
## Density & Generative Modeling

## Sample Quality

## Ablation Experiments

## Hybrid Modeling

# Conclusion
Invertible residual network는 강력한 생성 모델로서 사용될 수 있습니다. 제안된 모델은 다른 flow 기반의 모델에 비교할만한 또는 더 좋은 성능 보여줬습니다.
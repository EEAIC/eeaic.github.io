---
title: "ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning"
date: 2023-01-17 12:31
category: [Paper Review]
tags: [paper review]
math: true
published: false
img_path: "/assets/img/posts/2023-01-17-ClassMix"
---

# Introduction

저자들은 segmentation에서의 data augmentation 방법을 제안했습니다. 이를 ClassMix라고 명명했습니다. 그리고 이 방법은 semi-supervised semantic segmentation에서 활용될 수 있습니다. 

> 이 augmentation 방법은 어떤 image로부터 predicted class의 절반을 잘라 다른 이미지에 붙이는 것입니다. 

# Method

## ClassMix: Main Idea

ClassMix는 두 개의 unlabeled image를 입력으로 받고 합성된 augmented image와 이에 대응되는 artificial label을 생성합니다. 합성된 이미지는 어떤 이미지의 semantic class의 절반을 다른 이미지 위에 붙여 만들어집니다. 합성된 이미지는 새롭고 다양하지만, dataset의 다른 이미지와 여전히 유사합니다.

![Figure 1: ](ClassMix.png)
*Figure 1: ClassMix augmentation technique. 두 image $A$와 $B$는 unlabeled dataset에서 sampling. Image $A$의 prediction $S_A$에 기반하여, 이진 mask $M$이 생성됨.*

두 개의 unlabeled image, A와 B는 dataset에서 가져옵니다. 이 두 개의 이미지는 segmentation network $f_\theta$를 거쳐 prediction $S_A$와 $S_B$를 출력합니다. 이진 mask $M$은 argmaxed prediction $S_A$에서 나타난 클래스의 절반을 임의로 선택하여 생성됩니다. 선택된클래스를 mask에서는 1로 설정하고 반대의 경우에는 0으로 설정합니다. Mask의 값이 1인 image A의 pixel과 mask의 값이 1이 아닌 image B의 pixel을 통해 augmented image $X_A$를 만듭니다. 이러한 합성은 prediction $S_A$와 $S_B$에서도 똑같이 진행됩니다. 합성 방법의 특성상 artifact[^Artifact]가 나타날 수 있는데, 학습이 진행되면서 이러한 부분은 점점 적어지고 작아집니다.

> 추가적으로, consistency regulation은 불완전한 label에서도 좋은 성능을 산출해야 하는데, 이는 연구에서도 확인할 수 있습니다.

## ClassMix: Details

![Algroithm x](ClassMix_algorithm.png)
*Algorithm 1: ClassMix algorithm*

### Mean-Teacher Framework

ClassMix에서의 prediction에 대한 안정성을 향상시키기 위해, Mean Teacher Framework[^Mean_teacher]를 사용하였습니다. ClassMix에서 입력 이미지 $A$와 $B$에 대한 prediction을 생성할 때 $f_\theta$를 사용하는 것 대신에, 저자들은 $\theta$의 이전 값에 대한 exponential moving average인 $f_\theta^\prime$을 사용했습니다. 이러한 시간상의 ensembling 방법은 자원이 많이 들지 않고 갼단합니다. 그리고, 훈련동안에 더 안정적인 prediction을 보여주며 결과적으로 augmented image에 대해 더 안정적인 합성된 lablel을 생성합니다. Mixed image $X_A$는 network $f_\theta$을 학습시키는데 사용됩니다.

### Pseudo-labeled Output
ClassMix에 대해서 다른 중요한 부분은 augmented image에 대한 label을 생성할 때, 합성된 label $Y_A$은 "argmaxed" 되었다는 것입니다. 각 pixel에서 class에 대한 확률은 one-hot vector로 바꿉니다. 이 과정은 훈련중에 psudo-label을 생성할 때 사용됩니다. 이것은 semi-supervised learning에서 흔히 사용되는 기술입니다. 

> 왜냐하면, confi

ClassMix의 경우, pseudo-labeling은 경계에서의 불확실성을 제거하는 추가적인 목적이 있습니다. Mask $M$은 $A$에 대한 prediction으로부터 생성되기 때문에 mask의 경계는 semantic map의 결정 경계에 맞춰집니다. Segmentation task의 경우, class 경계에 가까울수록 어렵기 때문에 합성된 부분의 경계에 가까울수록 prediction이 불확실하다는 문제가 발생합니다. 이러한 문제는 label contamination이라 부릅니다.

![Figure x]()
*Figure 2: *

> $M$에 의해 선택된 class가 image $B$ 위에 붙여질 때, 그들의 인접한 context는 자주 바뀌게되고, 이는 나쁜 합성된 label을 

각 pixel에 대한 확률을 가장 확률의 높은 클래스에 대한 one-hot vector로 변경하기 때문에 Pseudo-labeling은 이러한 문제를 완화시킵니다. 이는, 생성된 label을 "sharpening(경계를 확실)"하게 되고, contamination을 초래하지 않습니다.

## Loss and Training

$$
L(\theta)=\mathbb E[\ell (f_\theta(X_L),Y_L)+\lambda \ell(f_\theta(X_A),Y_A)]
$$

위 식에서, $X_L$은 labeled image dataset에서 임의로 uniform하게 sampling된 image이고, $Y_L$은 그것에 대응되는 ground-truth semantic map입니다. 확률 변수 $X_A$와 $Y_A$는 각각 augmented image와 대응되는 artificial label 입니다. 마지막으로, $\lambda$는 supervised와 unsupervised 항의 균형을 잡기위한 hyper-parameter이고 $\ell$은 모든 pixel에 걸쳐 평균화된 cross-entorpy loss입니다.

$$
\ell(S,Y)=-\frac{1}{W \sdot H}\sum_{i=1}^W\sum_{j=1}^H(\sum_{c=1}^C Y(i,j,c) \sdot \log S(i,j,c))
$$

여기서 $W$와 $H$는 image의 넓이와 높이입니다. 그리고 $S(i,j,c)$와 $Y(i,j,c)$는 좌표 $i$, $j$의 pixel이 class $c$에 속할 확률입니다. $50\%$ labeled data와 $50\%$ augmented data로 구성된 batch를 사용하여 $\theta$를 훈련했습니다.

초기 network의 예측은 품질이 낮기 때문에, unsupervised 가중치 $\lambda$를 0에 가깝게 시작하는 것이 좋습니다. Network의 prediction의 성능이 향상됨에 따라, 이 가중치를 증가시킵니다. Artificial label의 각 pixel에서 class에 대해 가장 높은 확률값이 미리 결정된 임계값 $\tau$보다 큰 경우의 비율을 사용하여 augmented sample에 대한 $\lambda$를 설정합니다.

# Conclusion

이 논문에서는, 참신한 data augmentation technique인 ClassMix를 사용한 semi-supervised semantic segmentation을 위한 algorithm을 제안했습니다. ClassMix는 객체의 경계를 더 잘 반영하기 위해 network의 semantic prediction을 활용하여 unlabeled sample을 섞어서 augmented image와 artificial label을 생성하였습니다. 이러한 방법은 여러 dataset에서 기존 SOTA의 성능을 향상시켰습니다. 많은 segmentation algorithm이 data augmentation에 크게 의존하기 때문에, ClassMix는 향후 유용한 방법이 될 수 있습니다.

[^Artifact]: 산출물에서 보이는 결점(인공적이고, 인위적인 부분)

[^Mean_teacher]: Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Adv. Neural Inform. Process. Syst., pages 1195–1204, 2017.

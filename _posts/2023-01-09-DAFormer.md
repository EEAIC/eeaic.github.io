---
title: "DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation"
date: 2023-01-09 14:32
category: [Paper Review]
tags: [paper review, CVPR]
math: true
published: true
img_path: "/assets/img/posts/2023-01-09-DAFormer"
---
> Hoyer, Lukas, Dengxin Dai, and Luc Van Gool. "Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

# Introduction
지난 수년간, 신경망은 많은 computer vision task에서 놀라운 성능을 달성해왔습니다. 하지만, 이러한 task에는 학습에 적합한 매우 많은 양의 data annotation이 필요합니다. Semantic segmentation의 경우, 모든 pixel에 대해 라벨을 붙여야하므로 특히 매우 많은 비용이 들어갑니다. 예를 들어, Cityscapes dataset의 이미지 한장을 annotation 하기 위해서는 1.5시간이 소요됩니다. 심지어, 악천후의 경우에는 3.3시간이 소요됩니다. 이러한 문제를 피하기 위한 방법 중 하나는 syntheic(가상으로 제작된) data를 사용하는 것입니다. 하지만, 일반적으로 사용되는 CNN은 domain shift에 매우 민감하고 synthetic data에서 실제 data로 잘 일반화되지 않습니다. 이러한 문제는 source data에서 학습된 신경망을 target label을 사용하지 않고 target data에 adaptation(적응)시키는 UDA(Unsupervised Domain Adaptation)방법을 통해 해결할 수 있습니다.

UDA 환경에서 평가된 여러 다양한 semantic segmentation architecture에 대한 연구를 기반으로, 저자들은 DAFormer를 제안합니다. 이것은 CNN보다 강력하다고 여겨지는 transformer를 기반으로 합니다. 저자들은 이를 context-aware multi-level fusion과 결합히여 UDA에서의 성능을 더욱 향상시켰습니다. 저자들이 주장하기로는 DAFormer는 UDA semantic segmentation 분야에서 transformer의 잠재력을 밝힌 첫번째 연구입니다. 

더 복잡하고 강력한 architecture는 adaptation이 불안정하고 source domain에 과적합되기 쉽기 때문에, 저자들은 3가지 학습 기법을 제시하였습니다. 첫번째는 Rare Class Sampling(RCS)입니다. RCS는 흔한 class에 대해 self-training의 확증 편향 때문에 희귀 class에 대한 학습을 저해되는 문제를 해결하기 위해 source domain의 long-tail 분포를 고려하는 기법입니다. 희귀 class에 대한 이미지를 자주 샘플링하여, network가 pseudo-label의 quality를 향상시키고 확증 편향을 줄여 더 안정적으로 잘 학습할 수 있도록 합니다. 두번째로는 source domain에 대한 학습을 규제하기 위해 다양하고 표현력이 풍부한 ImageNet feature으로부터 지식을 distill(전달)하기 위한, Thing-Class ImageNet Feature Distance(FD)를 제안하였습니다. 여기서 ImageNet feature은 사물 class에 대하여 훈련되었기 때문에, 저자들은 FD를 사물class로 label이 지정된 이미지 영역으로 제한하였습니다. 세번째로, UDA를 위한 학습률 warmup을 새롭게 제안하였습니다. 학습 초기에 학습률을 의도한 값까지 선형적으로 증가시킴으로써, 학습 과정이 안정화되고 ImageNet feature가 시멘틱 세크멘테이션으로 더 잘 전달될 수 있다고 합니다.

# Methods
## Self-training(ST) for UDA
먼저, 여러 network architecture를 평가하기 위한 baseline UDA에 대해 설명하겠습니다. UDA에서, network $g_\theta$는 target label $\mathcal Y_T$를 사용하지 않고 target 이미지 $\mathcal X_T=\lbrace x_T^{(i)}\rbrace_{i=1}^{N_T}$에 대해 좋은 성능을 달성하기 위해 source domain 이미지 $\mathcal X_S = \lbrace x_S^{(i)} \rbrace_{i=1}^{N_S}$와 one-hot label $\mathcal Y_S = \lbrace y_S^{(i)} \rbrace_{i=1}^{N_S}$를 사용하여 훈련됩니다. Source domain에 대해 categorical cross-entropy(CE) loss로 naive하게 훈련하면, 일반적으로 network가 target domain으로 잘 일반화되지 않기 때문에 target 이미지에 대해 성능이 떨어집니다.

$$
\mathcal L_S^{(i)}=-\sum_{j=1}^{H \times W} \sum_{c=1}^C y_S^{(i,j,c)}\log g_\theta{(x_S^{(i)})^{(j,c)}}
$$

이러한 domain간의 차이를 해결하기 위해 제안된 여러 방법은 adversarial training과 self-training으로 그룹화할 수 있습니다. 이 논문에서는 adversarial training이 덜 안정적이고 현재 ST 방법이 더 좋은 성능을 내는 것으로 알려있기 때문에 ST 방법을 활용합니다. Source에서 target domain으로 지식을 더 잘 전달하기 위해, ST는 target domain data에 대한 pseudo-label을 생성하기 위해 teacher network $h_\phi$를 사용합니다.

$$
p^{(i,j,c)}_T=[c=\arg\max_{c^\prime}h_\phi(x^{(i)}_T)^{(j,c^\prime)}]
$$

위 식에서 $[\sdot]$은 아이버슨 괄호[^Iverson_bracket]를 나타냅니다. 여기서, teacher network는 gradient를 통한 backpropagation은 하지 않습니다. 또한, pseudo-label에 대한 quality/confidence estimate(추정치)를 생성합니다. 이를 계산하기 위해 maximum softmax probability 중 threshold $\tau$을 초과하는 pixel의 비율을 사용합니다.

$$
q_T^{(i)}=\frac{\sum_{j=1}^{H \times W}[\max_{c^\prime}h_\phi{(x_T^{(i)})}^{(j,c^\prime)}\gt \tau]}{H \sdot W}
$$

Pseudo-label과 quality estimate는 target domain에서 network $g_\theta$를 추가로 훈련하는데 사용됩니다.

$$
\mathcal L_T^{(i)}=-\sum_{j=1}^{H \times W} \sum_{c=1}^C q_T^{(i)}p_T^{(i,j,c)}\log g_\theta{(x_T^{(i)})^{(j,c)}}
$$

Pseudo-label은 online(훈련하는 과정에서) 혹은 offline(각각 따로 훈련하여)으로 생성할 수 있습니다. 여기서는 online ST를 선택했는데, 이는 하나의 훈련 단계만 있는 것이 덜 복잡한 설정이기 때문입니다. 이것은 우리가 다양한 network architecture를 비교하고 ablate할 때 중요합니다. Online ST에서 $h_\phi$는 $g_\theta$에 기반하여 업데이트됩니다. 일반적으로, $h_\phi$의 가중치는 각 training step 후 $g_\theta$의 exponentially moving average[^Mean_teacher]로 정해집니다.

$$
\phi_{t+1} \leftarrow \alpha\phi_t + (1-\alpha)\theta_t
$$

Semi-supervised learning과 UDA에서 ST는 teacher network $h_\phi$가 non-augmented target data을 사용하여 생성하는 동안, student network $g_\theta$가 augmented target data에 대해 훈련하는 것이 특히 효율적인 것으로 알려져 있습니다. 이 연구에서는, 기본적으로 DACS[^DACS] 방법을 따르고 더 많은 domain-robust feature를 학습시키기 위한 data augmentation으로 color jitter, Gaussian blur와 ClassMix를 사용합니다.

## DAFormer Network Architecture
이 연구 이전에는 대부분 오래된 DeepLabV2 architecture를 사용하여 그들의 연구를 평가했습니다. 이러한 이유로, 저자들은 좋은 supervised 성능뿐만아니라 좋은 DA 능력을 가진 network를 설계하였습니다.

인코더의 경우, 강력하면서도 robust한 network architecture를 목표로 합니다. Robustness는 domain-invariant feature에 대한 학습을 의미하기 때문에 좋은 DA 성능을 달성하기 위한 중요한 특성입니다. 최근 연구 결과에 따르면, transformer는 이러한 특성을 만족하기 때문에 UDA에 적합합니다. Transformer의 self-attention과 CNN 둘 다 weighted sum을 수행하지만, 그들의 weight는 서로 다르게 계산됩니다. CNN에서의 weight는 훈련 중에 학습되고 테스트 중에는 고정됩니다. 반면에, self-attention mechanism은 모든 token 쌍간의 similarity(유사성) 혹은 affinity(관련성)를 기반으로 동적으로 weight를 계산합니다. 결과적으로, self-attention mechanism은 CNN보다 더 adaptive한 일반적인 모델링 수단을 제공합니다.

이 논문에서는 기본적으로 semantic segmentation을 위해 설계된 Mix Transformer(MiT)[^MiT]를 따릅니다. 이미지는 semantic segmentation을 위한 detail을 보존하기 위해 $4 \times 4$(ViT[^ViT]에서는 $16 \times 16$ 크기였던 것 대신에) 크기의 작은 patch로 나누어집니다. 고해상도 feature에 대응하기 위해, self-attention block에서는 sequence reduction[^Sequence_reduction]이 사용됩니다. Transformer 인코더는 여러 단계의 feature map $F_i \in \mathbb{R}^{\frac{H}{2^{i+1}}\times\frac{W}{2^{i+1}}\times C_i}$을 생성하도록 설계되었습니다. Feature map의 downsampling은 local continuity를 보전하기 위해 overlapped patch merging[^MiT]을 통해 구현되었습니다.

Semantic segmentation에 대한 이전 연구들은 일반적으로 디코더에서는 local 정보들만 사용합니다. 이와 대조적으로, 저자들은 디코더에서 추가적인 context 정보를 활용하는 것을 제안했습니다. 왜냐하면, 이는 UDA에 도움이 되는 특성인 semantic segmentation의 robustness를 중가시켜주기 때문입니다. Bottleneck feature의 context 정보만을 고려하는 것 대신에, DAFormer는 서로 다른 단계의 인코더의 feature를 통해 context 정보를 고려합니다. 이전의 고해상도에서의 추출된 추가적인 feature가 semantic segmentation을 위한 중요한 하위 단계의 개념을 제공하기 때문입니다. DAFormer 디코더의 구조는 아래 그림에서 확인할 수 있습니다.

![Figure 1](DAFormer.png)
*Figure 1: Rare Class Sampling, Thing-Class Feature Distance 그리고 DAFormer로 구성된 UDA framework 개요*

Feature fusion을 하기 전에, $1 \times 1$ convolution을 통해 각 $F_i$를 같은 수의 채널 $C_e$로 임베딩하고 $F_1$의 크기로 feature를 upsampling 한 뒤 이를 concatenation 하였습니다. Context-aware feature fusion의 경우, ASPP[^ASPP]와 유사하지만 global average pooling 없이 서로 다른 dilation rate를 가지는 multiple parallel $3 \times 3$ depthwise separable convolution과  $1 \times 1$ convolution을 사용합니다. ASPP와는 달리, 이를 bottleneck feature $F_4$에만 적용하지 않고 모든 stacked multi-level feature를 융합하는데 적용하였습니다. Depthwise separable convolution은 일반적인 convolution보다 더 적은 수의 parameter를 가지고 있어 source domain에 대해 과적합을 줄일 수 있다는 장점이 있습니다.

## Training Strategies for UDA

### Rare Class Sampling(RCS)
저자들은 source dataset에서 희귀 class에 대한 UDA 성능이 여러 실험에서 유의미하게 다르다는 것을 관찰했습니다. Data 샘플링 순서가 랜덤 시드에 의존하기 때문에, 이러한 class가 훈련 중 서로 다른 시점에서 학습되거나 혹은 학습되지 않을 수 있습니다. 특정 class가 훈련 중에 늦게 배울수록, 훈련이 끝날 때 그것에 대한 성능은 더 나빠집니다. 이러한 이유로 저자들은 희귀 class가 포함된 관련 샘플이 무작위성으로 인해 훈련 후반에만 나타나는 경우 network는 나중에 학습을 시작하게 되고, 이는 network가 이미 자주 등장하는 class에 대한 강한 편향을 학습하여 매우 적은 샘플로 새로운 개념(class)을 '재학습'하기 어려울 것이라고 생각했습니다. 이러한 점은 ST의 teacher network의 확증 편향에 따라 더욱 강화됩니다.

이 문제를 해결하기 위해 저자들은 Rare Class Sampling(RCS)를 제안합니다. 이 방법은 희귀 class를 더 일찍 그리고 잘 학습하기 위해 source domain에서 더 자주 샘플링하는 방법입니다. Source dataset에서 각 class $c$의 빈도 $f_c$는 각 class $c$에 대한 pixel의 개수에 기반하여 계산할 수 있습니다.

$$
f_c=\frac{\sum^{N_S}_{i=1}\sum^{H \times W}_{j=1}[y_S^{(i,j,c)}]}{N_S \sdot H \sdot W} 
$$

어떤 class $c$에 대한 샘플링 확률 $P(c)$은 그것의 빈도 $f_c$에 대한 식으로 정의됩니다.

$$
P(c)=\frac{e^{(1-f_c)/T}}{\sum^C_{c^\prime = 1}e^{(1-f_{c^\prime})/T}}
$$

그러므로, 더 작은 빈도를 가지는 class는 더 높은 샘플링 확률을 가지게 됩니다. Temperture $T$는 분포의 smoothness를 제어합니다. $T$가 높을수록 uniform(균일) 분포에 가깝고, $T$가 낮을수록 작은 $f_c$을 가지는 희귀 class에 집중합니다. 각 source 샘플의 경우, 확률 분포 $c \sim P$으로부터 class를 샘플링하고 이러한 class를 포함하는 data의 부분집합에 대한 균일 분포 $x_S \sim \mathrm{uniform}(\mathcal X_{S,c})$으로부터 이미지를 샘플링합니다. 이 방식은 희귀 class를 포함하는 이미지를 오버샘플링할 수 있습니다. 희귀 class(작은 $f_c$)는 일반적으로 단일 이미지에서 여러 흔한 class(큰 $f_c$)와 함께 발생하므로, re-sampling된 class간의 균형을 맞추기 위해서는 흔한 class보다 희귀 class를 더 자주($P(c_{rare}) \gt P(c_{common})$) 샘플링하는 것이 좋습니다. 예를 들어, 흔한 class인 도로는 버스, 기차 또는 오토바이와 같은 희귀 class와 같이 나타나므로 이미 이러한 희귀 class로 샘플링할 때 이미 같이 다뤄지게 됩니다. Temperature $T$는 작은 $f_c$와 중간 $f_c$에 대해서 re-sampling된 class의 pixel의 수가 균형이 맞도록 선택됩니다.

### Thing-Class ImageNet Feature Distance(FD)
일반적으로, semantic segmentation 모델 $g_\theta$는 의미있는 일반적인 feature로부터 학습을 시작하기 위해 ImageNet 분류를 위해 pretrain된 weight로 초기화합니다. ImageNet에는 UDA에서 종종 구별하기 어려운 기차나 버스 같이 high-level semantic class에 대한 일부 실제 이미지도 포함되어 있다는 점을 고려하면, 저자들은 ImageNet feature가 일반적인 pretrain의 이점을 넘어 유용한 가이드를 제공할 수 있다고 생각했습니다. 특히, DAFormer는 훈련의 시작에는 몇몇 구별하기 어려운 class를 잘 구분하지만 훈련이 진행되면서 나중에는 해당 class를 잘 구분하지 못하는 문제를 관찰했습니다. 따라서, pretrain을 통한 ImageNet의 유용한 feature가 $L_S$에 의해 손상되고 모델이 synthetic source data에 과적합되었다고 볼 수 있습니다. 

이러한 문제를 막기 위해, semantic segmentation UDA 모델 $g_{\theta}$의 bottleneck feature $F_\theta$와 ImageNet 모델의 bottleneck feature $F_{ImageNet}$간의 Feature Distance(FD)에 기반하여 모델을 규제합니다.

$$
d(i,j) = \Vert F_{ImageNet}(x^{(i)}_S)^{(j)}-F_\theta(x^{(i)}_S)^{(j)}\Vert_2
$$

그러나, ImageNet 모델은 주로 사물 class(자동차나 얼룩말처럼 윤곽이 뚜렷한 물체)에 대해 학습됩니다. 그러므로, FD loss는 이진 마스크 $M_{things}$를 통해 사물 class $C_{things}$가 포함된 이미지 영역에 대해서만 계산되어야 합니다.

$$
\mathcal{L}^{(i)}_{FD}=\frac{\sum^{H_F \times W_F}_{j=1}d^{(i, j)}\sdot M^{(i, j)}_{things}}{\sum_jM^{(i, j)}_{things}}
$$

여기서 마스크는 downscaled label $y_{S,small}$로부터 얻어집니다.

$$
M^{(i, j)}_{things}=\sum^C_{c^\prime=1} y^{i,j,c^\prime}_{S, small}\sdot [c^\prime \in \mathcal{C}_{things}]
$$

Label을 bottleneck feature 크기로 다운샘플링하기 위해서, $\frac{H}{H_F} \times \frac{W}{W_F}$크기의 patch를 통해 average pooling을 class 채널에 적용하고 class는 일정 비율 $r$을 초과할 때 유지됩니다.

$$
y^c_{S,small}=[\mathrm{AvgPool}(y^c_S,H/H_F,W/W_F)\gt r]
$$

이것은 이미지에서 주된 사물 class에 대한 bottleneck feature pixel만 feature distance에  고려되도록 합니다.

전체적인 UDA loss $\mathcal{L}$ 은 제시된 loss 요소들의 weighted sum입니다.

$$
\mathcal L= \mathcal L_S + \mathcal L_T + \lambda_{FD} \mathcal L_{FD}
$$

### Learning Rate Warmup for UDA
훈련 시작시 선형적으로 학습률을 warmup하는 것은 CNN과 transformer 둘 다에서 성공적으로 사용되어져 왔습니다. 왜냐하면, 훈련 시작시 adaptive 학습률의 큰 분산은 gradient 분포를 왜곡하는 것을 방지하여 network의 일반화능력을 향상시키기 때문입니다.[^Adaptive_lr] 저자들은 이러한 학습률 warmup을 UDA에 새롭게 도입하였습니다. 저자들은 ImageNet에 대해 pretrain된 feature를 왜곡하는 것은 실제 domain에 대한 유용한 가이드를 잃는 것이기 때문에 UDA에 특히 중요함을 상정합니다. Iteration $t_{warm}$까지의 warmup 기간 동안, iteration $t$에서의 학습속도는 $\eta_t =\eta_{base} \sdot t/t_{warm}$으로 설정됩니다.

# Conclusion
이 논문은 transformer encoder와 context-aware fusion decoder를 기반으로 하여 UDA에 적합한 architecture를 제안했습니다. 이 architecture는 UDA에 대하여 transformer의 가능성을 보여줬습니다. 추가적으로 학습의 안정화와 규제를 위한, 더욱이 DAFormer의 성능을 향상시키는 3가지의 학습 기법을 제안했습니다. 전체적으로, DAFormer는 UDA에서 큰 성능 향상을 보여줬습니다. 여기서 제시된 DAFormer는 이후의 논문인 HRDA, MIC에서도 사용되는 만큼 향후 연구를 이해하는데 필수적인 논문이라 생각됩니다.

# Footnote
[^Iverson_bracket]: Wikipedia contributors. (2022, October 24). Iverson bracket. Wikipedia. [https://en.wikipedia.org/wiki/Iverson_bracket](https://en.wikipedia.org/wiki/Iverson_bracket)

[^DACS]: Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, and Lennart Svensson. DACS: Domain Adaptation via Crossdomain Mixed Sampling. In IEEE Winter Conf. on Applications of Comput. Vis., pages 1379–1389, 2021.

[^MiT]: Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. In Adv. Neural Inform. Process. Syst., 2021.

[^ViT]: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020.

[^Sequence_reduction]: Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Int. Conf. Comput. Vis., pages 568–578, 2021.

[^ASPP]: Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Eur. Conf. Comput. Vis., pages 801–818, 2018.

[^Mean_teacher]: Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Adv. Neural Inform. Process. Syst., pages 1195–1204, 2017.

[^Adaptive_lr]: Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In Int. Conf. Learn. Represent., 2019.
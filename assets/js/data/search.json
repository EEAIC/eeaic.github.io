[ { "title": "[Troubleshooting] A file name cannot contain any of the following characters", "url": "/posts/Windows-filename/", "categories": "Windows", "tags": "windows, troubleshooting, unicode", "date": "2023-01-25 16:45:00 +0900", "snippet": "Windows에서 파일을 일부 문자를 포함해서 저장하려고 시도하면 아래와 같은 이유로 저장할 수 없습니다.Figure 1: 파일 이름 입력시 사용할 수 없는 문자를 입력하면 뜨는 경고창Troubleshooting\\ / : * ? \" &lt; &gt; |를 사용하는 것은 불가능하므로 이와 유사한 문자를 찾아 사용해야 합니다. 여기서는 두가지 방법을 소개합니다. Unicode Character Table 혹은 다른 여러 유니코드 사이트에서 사용불가 문자와 비슷한 문자를 찾을 수 있습니다. 사용불가 문자 유사한 문자 유니코드 / ∕ U+2215 : ꞉ U+A789 * ＊ ∗ U+FF0A U+2217 ? ？ U+FF1F “ ” “ ” U+201C U+201D &lt; ＜ U+FF1C &gt; ＞ U+FF1E | ⏐ U+23D0 한글 ㄱ을 누른 뒤 한자키를 눌러 뜨는 창에서 사용불가 문자와 비슷한 문자를 찾을 수 있습니다. Figure 2: ㄱ과 한자키를 누르면 뜨는 창Reference unix - How to get a file in Windows with a colon in the filename? - Stack Overflow" }, { "title": "HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation", "url": "/posts/HRDA/", "categories": "Paper Review", "tags": "paper review, ECCV", "date": "2023-01-13 12:53:00 +0900", "snippet": " Hoyer, L., Dai, D., &amp; Van Gool, L. (2022). HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation. Proceedings of the European Conference on Computer Vision (ECCV).Introduction이 연구는 UDA에 관한 연구입니다. Target dataset에 대해 annotation을 하지 않기 위해, network는 이미 존재하거나 annotation 하기 쉬운 source dataset을 통해 훈련됩니다. 그러나, neural network은 domain shift에 매우 민감합니다. 이러한 문제는 UDA에서 source data에 대해 훈련된 network를 unlabeld target image에 adapt하여 해결합니다.UDA 방법은 일반적인 supervised learning 보다 더 많은 GPU 메모리가 필요합니다. 왜냐하면, UDA는 주로 여러 domain의 이미지, 추가적인 network와 GPU 메모리에 상당한 영향을 주는 추가적인 loss를 필요로 합니다. 그러므로, 대부분의 UDA semantic segmentation 방법은 GPU 메모리 제약 때문에서 이미지를 downscaling하여 사용합니다. 그에 반해 대부분의 supervised 방법은 downscaling 없이 그대로 사용합니다.저해상도 입력에서의 예측은 종종 먼 거리의 신호동과 같은 작은 객체를 인식하는 데 실패하고 먼 거리의 보행자의 팔다리와 같은 fine segmentation detail을 유지하지 못합니다. 그러나 전체 고해상도 image로 naively 학습하는 것은 GPU 메모리 요구량에 상당한 영향을 주기 때문에 사용하기 어렵습니다. 일반적인 해결책은 image의 random crop을 통해 훈련시키는 것입니다. HR crop을 이용하여 훈련하면 작은 객체을 adapt하고 segmentation detail을 유지할 수 있습니다. 이 방법은 UDA의 경우에 context information과 scene layout이 종종 domain-robust 해야 하기 때문에 큰 단점으로 작용합니다. HR 입력은 작은 객체를 adapt하는데 필수적인 반면, 좁은 보도와 같은 크기 큰 객체 영역을 adapt하기에는 단점이 있습니다. HR은 이러한 영역에 대해 종종 너무 detail하며, 특정 domain에만 국한된 feature를 가지고 있습니다. 이러한 점은 UDA에서 불리하게 작용합니다. LR 입력은 이러한 feature를 감추고, domain에 걸쳐 넓은 영역을 인식하기 위한 충분한 detail을 제공합니다.어느정도 감당 가능한 GPU 메모리 공간을 유지하면서 두 접근 방식의 강점을 효율적으로 결합하기 위해, 저자들은 HRDA를 제안합니다. 먼저, HRDA는 특정 domain에 국한된 HR texture를 혼동하지 않도록 큰 객체에 대해 adapt시키기 위해 큰 크기의 LR context crop을 사용합니다. 또한, HR detail은 long-range dependency에 대해 중요하지 않다고 생각하기 때문에 long-range context를 학습시키기 위해서도 LR context crop을 사용합니다. 둘째, HRDA는 작은 객체를 adapt하고 segementation detail을 유지하기 위해 context crop 내의 영역에서 작은 HR detail crop을 사용합니다. 또한, long-range context information은 segmentation detail을 학습하는데 부가적인 역할만을 한다고 생각하기 때문에 segmentation detail을 보존하기 위해 HR detail crop을 사용합니다. 이러한 방식을 통해, GPU 메모리 사용량은 크게 줄이면서 큰 crop 크기와 고해상도의 이점을 유지합니다. 이미지의 내용에 따라 LR context crop과 HR detail crop간의 중요도가 달라진다는 점을 고려하기 위해, HRDA는 input-dependent scale attention을 사용하여 이 둘을 합칩니다. Attention은 LR 및 HR의 예측이 얼마나 신뢰할 수 있는지에 대해 결정하는 법을 학습합니다. Supervised learning을 위한 이전 multi-resolution framework는 전체 LR 및 HR 이미지를 사용하기 때문에서, GPU 메모리 제약으로 인해 UDA에서 naive하게 적용될 수 없습니다. 또한, HRDA을 target domain으로 adapt하기 위해 여러 해상도가 합쳐진 pseudo-label을 통해 훈련을 할 수 있습니다. 더욱이, 서로 다른 context에 대해 detail pseudo-label의 robustness를 증가시키기 위해 pseudo-label은 overalpping sliding window mechanism을 사용하여 생성됩니다.MethodContext and Detail CropGPU 메모리 제한사항 때문에, UDA에서 요구되는 여러 domain에서 전체 크기의 고해상도/여러 해상도의 이미지 입력과 추가적인 network 와 loss를 통해 SOTA UDA 모델을 학습하는 것은 불가능합니다. 그러므로 대부분의 이전 연구에서는 LR 입력만을 사용합니다. 그러나, HR 입력은 작은 객체를 인식하고 fine segmentation 경계를 생성하는데 중요합니다. HR 입력을 이용하기 위해선, random cropping이 가능한 대안이 될 수 있습니다. 하지만, random cropping은 길 위에 차가 있는 장면이나 자전거 위에 사람이 타고 있는 장면과 같이 context relation은 종종 doamin-invariant하기 때문에 UDA에서 중요한 scene layout과 long-range dependency에 대한 context-aware semantic segmentation을 학습하는데 제한이 될 수 있습니다. 고해상도와 long-range context를 모두 사용하여 학습하기 위해서, 저자들은 서로 다른 해상도에서 대해서 서로 다른 crop 크기를 결합하는 것을 제안합니다. 예를 들어, 아래 그림과 같이 LR context crop과 작은 HR detail crop을 결합합니다.Figure 1: (a) 저해상도(LR, low-resolution)와 고해상도(HR, high-resolution) detail crop을 이용한 multi-resolution 훈련. Detail crop의 prediction은 학습된 scale attention을 통해 context prediction에서 detail crop이 잘린 부위와 대응되는 영역과 합쳐짐. (b) Pseudo-label 생성의 경우, 여러 detail crop을 통해 전체 context crop을 생성하기 위해 overlapping slide inference를 사용하여 생성됨. Pseudo-label은 HR pred. $\\hat y_{c,HR}^T$와 LR pred. $\\hat y_c^T$를 (a)와 유사하게 full attention을 통해 합침.Context crop은 long-range context를 학습시키기 위한 large crop을 제공하는 것이 목적입니다. Detail crop은 HR을 통해 작은 객체를 인식하고 fine segmentation detail을 생성하는 것이 목적입니다. Model validation동안에는 전체 이미지에 대해 segmentation을 하기 위해 overlapping sliding window inference가 사용되었습니다.Context crop $x_c \\in \\mathbb R^{h_c \\times w_c \\times 3}$는 원래 HR 이미지 $x_{HR} \\in \\mathbb R^{H \\times W \\times 3}$에 대한 cropping과 $s \\ge 1$인 factor를 통해 bilinear downsampling을 통해 얻어집니다.\\[x_{c, HR}=x_{HR}[b_{c,1} \\ratio b_{c,2},\\, b_{c,3} \\ratio b_{c,4}], \\quad x_c=\\zeta(x_{c, HR},\\,1/s)\\]Crop bounding box $b_c$는 이후에 합치는 과정에서 정확히 정렬하기 위해서 $o \\ge 1$인 $k=s\\sdot o$에 의해 정확히 나누어 떨어지도록 이미지 크기 내에서 discrete uniform distribution을 통해 sampling됩니다.\\[b_{c,1} \\sim \\{0, (H-sh_c)/k \\} \\sdot k, \\quad b_{c,2} = b_{c,1} + sh_c, \\\\b_{c,3} \\sim \\{0, (W-sw_c)/k \\} \\sdot k, \\quad b_{c,4} = b_{c,3} + sw_c,\\]Detail crop $x_d \\in \\mathbb R^{h_d \\times w_d \\times 3}$은 context와 detail에서의 예측을 나중에 합치기 위해서 context crop region안에서 randonm하게 crop됩니다.\\[x_d = x_{x_c, HR}[b_{d, 1} \\ratio b_{d, 2},\\, b_{d, 3} \\ratio b_{d, 4}], \\\\b_{d,1} \\sim \\{0, (sh_c -h_d)\\} \\sdot k, \\quad b_{d, 2} = b_{d,1} + h_d,\\\\b_{d,3} \\sim \\{0, (sw_c - w_d)\\} \\sdot k, \\quad b_{d, 4} = b_{d, 3} + w_d\\]저자들은 동일한 차원의 context와 detail crop을 사용했습니다. 즉, $h_c = h_d$이고 $w_c=w_d$입니다. LR을 사용한 이전 UDA 방법에 따라 downscale scale factor는 $s = 2$로 설정했습니다. 이는 context crop은 detail crop에 비해 4배 많은 내용을 담는 것을 의미합니다.Feature encoder $f^E$와 semantic decoder $f^S$를 사용하여, context semantic segmentation $\\hat y_c=f^S(f^E(x_c)) \\in \\mathbb R ^{\\frac{h_c}{o} \\times \\frac{w_c}{o}\\times C}$과 detail semantic segmentation $\\hat y_d=f^S(f^E(x_d)) \\in \\mathbb R^{\\frac{h_d}{o} \\times \\frac{w_d}{o}\\times C}$을 예측합니다. HR과 LR입력에 대해 같은 network $f_E$와 $f_S$를 사용합니다. 이것은 메모리 사용량을 줄일뿐만 아니라 서로 다른 해상도에 대한 network의 robustness를 증가시킵니다.Multi-Resolution FusionHR detail crop은 pole이나 distant pedestrian과 같이 작은 객체를 segmentation하는데 매우 적합합니다. 반면에, long-range dependecy를 잡아내는 능력은 부족합니다. LR context crop은 이와 반대입니다. 그러므로, 저자들은 context와 detail crop에서의 예측을 신뢰할 이미지 영역을 예측하기 위한 학습가능한 scale attention을 사용하여 두 crop을 통한 예측을 합칩니다. 또한, scale attention은 객체를 좀 더 적합한 scale에서 adpat할 수 있는 이점을 제공합니다. 예를 들어, 작은 객체는 HR에서 adapt하기 쉬운 반면, 큰 객체는 LR에서 adapt하기 쉽습니다. 왜냐하면, 객체의 외관이 해상도가 너무 높은면 network가 domain-specific detailed texture에 과적합될 수 있기 때문입니다.Scale attention decoder $f_A$는 LR context와 HR detail prediction이 어느 정도 비중을 두고 신뢰할 수 있는지 판단하기 위해 scale attention $a_c=\\sigma(f_A(f_E(x_c))) \\in [0, 1]^{\\frac{h_c}{o} \\times \\frac{w_c}{o} \\times C}$을 학습합니다. Sigmoid 함수 $\\sigma$는 weight가 $[0, 1]$안에 있도록 보장합니다. 여기서, 1은 HR detail crop에 초점을 맞추는 것을 의미합니다. Attention은 context crop으로부터 예측됩니다. 왜냐하면, context crop이 scene layout(larger context)를 더 잘 파악할 수 있기 때문입니다. Output stride $o$로 인해 입력이 예측보다 작기 때문에서, 다음 단계에서 crop 좌표의 크기는 조정됩니다. Detail crop $c_d$의 바깥의 detail prediction이 없기 때문에 attention은 0으로 설정됩니다.\\[a_c^\\prime \\in \\mathbb R^{\\frac{h_c}{o}\\times\\frac{w_c}{o}}, \\quad a_c^\\prime=\\begin{cases} a_c(i,j) &amp;\\text{if} \\space \\frac{b_{d,1}}{s\\sdot o} \\le i \\lt \\frac{b_{d,2}}{s \\sdot o} \\land \\frac{b_{d,3}}{s \\sdot o} \\le j \\lt \\frac{b_{d,4}}{s \\sdot o} \\\\ 0 &amp;\\text{otherwise} \\end{cases}\\]Detail crop은 0으로 padding하여 $\\frac{sh_c}{o} \\times \\frac{sw_c}{o}$의 크기로(upsampled) context crop과 일치하게 정렬됩니다.\\[\\hat y_d^\\prime(i,j)=\\begin{cases} \\hat y_d(i-\\frac{b_{d,1}}{o},j-\\frac{b_{d,3}}{o}) &amp;\\text{if} \\space \\frac{b_{d,1}}{o} \\le i \\lt \\frac{b_{d,2}}{o} \\land \\frac{b_{d,3}}{o} \\le j \\lt \\frac{b_{d,4}}{o}\\\\ 0 &amp;\\text{otherwise} \\end{cases}\\]최종적으로 다양한 scale의 예측은 attention-weighted sum을 통해 합쳐집니다.\\[\\hat y_{c,F} =\\zeta((1-a_c^\\prime) \\odot \\hat y_c, s) + \\zeta(a_c^\\prime, s) \\sdot \\hat y_{d}^\\prime\\]Encoder, segmentation head 그리고 attention head는 합쳐진 multi-scale 예측과 detail crop 예측을 통해 훈련됩니다.\\[\\mathcal L_{HRDA}^S=(1-\\lambda)\\mathcal L_{ce}(\\hat y_{c,F}^{S},y_c^S,1) + \\lambda_d\\mathcal L_{ce}(\\hat y_d^S,y_d^S,1)\\]Detail crop에 대한 항이 존재하는 이유는 비록 attention이 해당 영역에서 context crop에 더 가중을 두더라도 HR 입력에 대해 더 robust한 feature를 학습하는데 도움이 되기 때문입니다. Target loss $\\mathcal L_{HRDA}^T$는 아래와 같습니다.\\[\\mathcal L_{HRDA}^T = (1-\\lambda_d)\\mathcal L_{ce}(\\hat y_{c,F}^T, p_{c,F}^T, q_{c,F}^T) + \\lambda_d \\mathcal L_{ce}(\\hat y_d^T, p_d^T, q_d^T)\\]pseudo-label의 생성에도 multi-resolution fusion을 사용하였습니다. 이로 인해, pseudo-label을 예측할 때도 scale attention을 통해 더 적합한 해상도에 초점을 맞춥니다. Pseudo-label은 덜 적합한 해상도(예: 작은 객체에 대한 LR)로 모델을 훈련시키는데에도 사용되며 이는 작은 객체와 큰 객체 모두에 대한 robustness가 향상된다고 합니다. 필자가 생각하기로는 HRDA prediction $\\hat y_{c,F}^T$중 HR이 합쳐지지 않은 영역은 context prediciton이며 이 부분은 작은 객체에 대해 덜 적합한 해상도라 생각됩니다. 따라서 이 부분에 대한 loss가 덜 적합한 해상도로 모델을 훈련시키는데 사용되었다고 볼 수 있을 것 같습니다.Pseudo-Label Generation with Overlapping Sliding WindowSelf-training의 경우, context crop $x_{c,HR}^T$를 위한 고품질의 HRDA pseudo-label $p_{c,F}^T$을 생성할 필요가 있습니다. LR prediction $\\hat y_c^T$과 HR prediction $\\hat y_{c,HR}^T$을 full scale attention $a_c^T$를 통해 합쳐서 HRDA prediction $\\hat y_{c,F}^T$을 생성합니다.\\[\\hat y_{c,F}^T=\\zeta((1-a_c^T) \\odot \\hat y_c^T,s)+\\zeta(a_c^T,s) \\odot \\hat y_{c,HR}^T\\]비록 큰 HR network 입력은 훈련중에는 문제가 될 수 있지만, pseudo-label을 추론할 때는 역전파 과정이 없기 때문에 큰 문제가 되지 않습니다. 하지만, DAFormer 혹은 다른 Vision Transformer도 마찬가지로 훈련과 추론의 입력 크기가 동일할 때 (implicit) positional embedding이 잘 작동합니다. 그러므로, $h_d \\times w_d$크기의 sliding window를 HR context crop $x_{c,HR}^T$에 적용하여 HR prediction $\\hat y_{c,HR}^T$를 추론합니다. 여기서, window는 서로 다른 context을 통해 overlapping prediction을 생성하기 위해 $h_d/2 \\times w_d/2$의 stride로 shift됩니다. Sliding window를 통한 crop images의 생성은 batch를 통해 병렬 처리 할 수 있습니다. 즉, GPU에서 매우 효율적으로 계산할 수 있습니다.Model validation이나 deployment의 경우, 전체 이미지 $x_{HR}$에 대한 full-scale HRDA semantic segmentation $\\hat y_{F,HR}$가 필요합니다. Context crop은 일반적으로 전체 이미지보다 작기 때문에, $\\hat y_{F,HR}$은 전체 이미지 $x_{HR}$에 대해 $sh_c \\times sw_c$의 크기와 $sh_c/2 \\times sw_c/2$의 stride으로 overlapping sliding window를 통해 생성합니다.Conclusion이 연구에서는 감당할 수 있는 GPU 메모리 공간을 유지하면서 학습 가능한 scale attention을 통하여 작은 HR detail crop과 큰 LR context crop의 이점을 결합한 UDA를 위한 multi-resolution 접근인 HRDA를 제안했습니다. 이것은 다양한 UDA 방법과 결합할 수 있고 일관성있게 상당한 성능 향상을 달성했습니다. 전체적으로, HRDA는 GTA5 $\\rightarrow$ Cityscapes에서 73.8 mIoU와 SYNTHIA $\\rightarrow$ Cityscapes에서 65.8 mIoU의 전례 없는(unprecedented) 성능을 각각 달성하였습니다. 이는 이전 SOTA에 비해 각각 +5.5 mIoU와 +4.9 mIoU가 올라간 수치입니다." }, { "title": "DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation", "url": "/posts/DAFormer/", "categories": "Paper Review", "tags": "paper review, CVPR", "date": "2023-01-09 14:32:00 +0900", "snippet": " Hoyer, Lukas, Dengxin Dai, and Luc Van Gool. “Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.Introduction지난 수년간, 신경망은 많은 computer vision task에서 놀라운 성능을 달성해왔습니다. 하지만, 이러한 task에는 학습에 적합한 매우 많은 양의 data annotation이 필요합니다. Semantic segmentation의 경우, 모든 pixel에 대해 라벨을 붙여야하므로 특히 매우 많은 비용이 들어갑니다. 예를 들어, Cityscapes dataset의 이미지 한장을 annotation 하기 위해서는 1.5시간이 소요됩니다. 심지어, 악천후의 경우에는 3.3시간이 소요됩니다. 이러한 문제를 피하기 위한 방법 중 하나는 syntheic(가상으로 제작된) data를 사용하는 것입니다. 하지만, 일반적으로 사용되는 CNN은 domain shift에 매우 민감하고 synthetic data에서 실제 data로 잘 일반화되지 않습니다. 이러한 문제는 source data에서 학습된 신경망을 target label을 사용하지 않고 target data에 adaptation(적응)시키는 UDA(Unsupervised Domain Adaptation)방법을 통해 해결할 수 있습니다.UDA 환경에서 평가된 여러 다양한 semantic segmentation architecture에 대한 연구를 기반으로, 저자들은 DAFormer를 제안합니다. 이것은 CNN보다 강력하다고 여겨지는 transformer를 기반으로 합니다. 저자들은 이를 context-aware multi-level fusion과 결합히여 UDA에서의 성능을 더욱 향상시켰습니다. 저자들이 주장하기로는 DAFormer는 UDA semantic segmentation 분야에서 transformer의 잠재력을 밝힌 첫번째 연구입니다.더 복잡하고 강력한 architecture는 adaptation이 불안정하고 source domain에 과적합되기 쉽기 때문에, 저자들은 3가지 학습 기법을 제시하였습니다. 첫번째는 Rare Class Sampling(RCS)입니다. RCS는 흔한 class에 대해 self-training의 확증 편향 때문에 희귀 class에 대한 학습을 저해되는 문제를 해결하기 위해 source domain의 long-tail 분포를 고려하는 기법입니다. 희귀 class에 대한 이미지를 자주 샘플링하여, network가 pseudo-label의 quality를 향상시키고 확증 편향을 줄여 더 안정적으로 잘 학습할 수 있도록 합니다. 두번째로는 source domain에 대한 학습을 규제하기 위해 다양하고 표현력이 풍부한 ImageNet feature으로부터 지식을 distill(전달)하기 위한, Thing-Class ImageNet Feature Distance(FD)를 제안하였습니다. 여기서 ImageNet feature은 사물 class에 대하여 훈련되었기 때문에, 저자들은 FD를 사물class로 label이 지정된 이미지 영역으로 제한하였습니다. 세번째로, UDA를 위한 학습률 warmup을 새롭게 제안하였습니다. 학습 초기에 학습률을 의도한 값까지 선형적으로 증가시킴으로써, 학습 과정이 안정화되고 ImageNet feature가 시멘틱 세크멘테이션으로 더 잘 전달될 수 있다고 합니다.MethodsSelf-training(ST) for UDA먼저, 여러 network architecture를 평가하기 위한 baseline UDA에 대해 설명하겠습니다. UDA에서, network $g_\\theta$는 target label $\\mathcal Y_T$를 사용하지 않고 target 이미지 $\\mathcal X_T=\\lbrace x_T^{(i)}\\rbrace_{i=1}^{N_T}$에 대해 좋은 성능을 달성하기 위해 source domain 이미지 $\\mathcal X_S = \\lbrace x_S^{(i)} \\rbrace_{i=1}^{N_S}$와 one-hot label $\\mathcal Y_S = \\lbrace y_S^{(i)} \\rbrace_{i=1}^{N_S}$를 사용하여 훈련됩니다. Source domain에 대해 categorical cross-entropy(CE) loss로 naive하게 훈련하면, 일반적으로 network가 target domain으로 잘 일반화되지 않기 때문에 target 이미지에 대해 성능이 떨어집니다.\\[\\mathcal L_S^{(i)}=-\\sum_{j=1}^{H \\times W} \\sum_{c=1}^C y_S^{(i,j,c)}\\log g_\\theta{(x_S^{(i)})^{(j,c)}}\\]이러한 domain간의 차이를 해결하기 위해 제안된 여러 방법은 adversarial training과 self-training으로 그룹화할 수 있습니다. 이 논문에서는 adversarial training이 덜 안정적이고 현재 ST 방법이 더 좋은 성능을 내는 것으로 알려있기 때문에 ST 방법을 활용합니다. Source에서 target domain으로 지식을 더 잘 전달하기 위해, ST는 target domain data에 대한 pseudo-label을 생성하기 위해 teacher network $h_\\phi$를 사용합니다.\\[p^{(i,j,c)}_T=[c=\\arg\\max_{c^\\prime}h_\\phi(x^{(i)}_T)^{(j,c^\\prime)}]\\]위 식에서 $[\\sdot]$은 아이버슨 괄호1를 나타냅니다. 여기서, teacher network는 gradient를 통한 backpropagation은 하지 않습니다. 또한, pseudo-label에 대한 quality/confidence estimate(추정치)를 생성합니다. 이를 계산하기 위해 maximum softmax probability 중 threshold $\\tau$을 초과하는 pixel의 비율을 사용합니다.\\[q_T^{(i)}=\\frac{\\sum_{j=1}^{H \\times W}[\\max_{c^\\prime}h_\\phi{(x_T^{(i)})}^{(j,c^\\prime)}\\gt \\tau]}{H \\sdot W}\\]Pseudo-label과 quality estimate는 target domain에서 network $g_\\theta$를 추가로 훈련하는데 사용됩니다.\\[\\mathcal L_T^{(i)}=-\\sum_{j=1}^{H \\times W} \\sum_{c=1}^C q_T^{(i)}p_T^{(i,j,c)}\\log g_\\theta{(x_T^{(i)})^{(j,c)}}\\]Pseudo-label은 online(훈련하는 과정에서) 혹은 offline(각각 따로 훈련하여)으로 생성할 수 있습니다. 여기서는 online ST를 선택했는데, 이는 하나의 훈련 단계만 있는 것이 덜 복잡한 설정이기 때문입니다. 이것은 우리가 다양한 network architecture를 비교하고 ablate할 때 중요합니다. Online ST에서 $h_\\phi$는 $g_\\theta$에 기반하여 업데이트됩니다. 일반적으로, $h_\\phi$의 가중치는 각 training step 후 $g_\\theta$의 exponentially moving average2로 정해집니다.\\[\\phi_{t+1} \\leftarrow \\alpha\\phi_t + (1-\\alpha)\\theta_t\\]Semi-supervised learning과 UDA에서 ST는 teacher network $h_\\phi$가 non-augmented target data을 사용하여 생성하는 동안, student network $g_\\theta$가 augmented target data에 대해 훈련하는 것이 특히 효율적인 것으로 알려져 있습니다. 이 연구에서는, 기본적으로 DACS3 방법을 따르고 더 많은 domain-robust feature를 학습시키기 위한 data augmentation으로 color jitter, Gaussian blur와 ClassMix를 사용합니다.DAFormer Network Architecture이 연구 이전에는 대부분 오래된 DeepLabV2 architecture를 사용하여 그들의 연구를 평가했습니다. 이러한 이유로, 저자들은 좋은 supervised 성능뿐만아니라 좋은 DA 능력을 가진 network를 설계하였습니다.인코더의 경우, 강력하면서도 robust한 network architecture를 목표로 합니다. Robustness는 domain-invariant feature에 대한 학습을 의미하기 때문에 좋은 DA 성능을 달성하기 위한 중요한 특성입니다. 최근 연구 결과에 따르면, transformer는 이러한 특성을 만족하기 때문에 UDA에 적합합니다. Transformer의 self-attention과 CNN 둘 다 weighted sum을 수행하지만, 그들의 weight는 서로 다르게 계산됩니다. CNN에서의 weight는 훈련 중에 학습되고 테스트 중에는 고정됩니다. 반면에, self-attention mechanism은 모든 token 쌍간의 similarity(유사성) 혹은 affinity(관련성)를 기반으로 동적으로 weight를 계산합니다. 결과적으로, self-attention mechanism은 CNN보다 더 adaptive한 일반적인 모델링 수단을 제공합니다.이 논문에서는 기본적으로 semantic segmentation을 위해 설계된 Mix Transformer(MiT)4를 따릅니다. 이미지는 semantic segmentation을 위한 detail을 보존하기 위해 $4 \\times 4$(ViT5에서는 $16 \\times 16$ 크기였던 것 대신에) 크기의 작은 patch로 나누어집니다. 고해상도 feature에 대응하기 위해, self-attention block에서는 sequence reduction6이 사용됩니다. Transformer 인코더는 여러 단계의 feature map $F_i \\in \\mathbb{R}^{\\frac{H}{2^{i+1}}\\times\\frac{W}{2^{i+1}}\\times C_i}$을 생성하도록 설계되었습니다. Feature map의 downsampling은 local continuity를 보전하기 위해 overlapped patch merging4을 통해 구현되었습니다.Semantic segmentation에 대한 이전 연구들은 일반적으로 디코더에서는 local 정보들만 사용합니다. 이와 대조적으로, 저자들은 디코더에서 추가적인 context 정보를 활용하는 것을 제안했습니다. 왜냐하면, 이는 UDA에 도움이 되는 특성인 semantic segmentation의 robustness를 중가시켜주기 때문입니다. Bottleneck feature의 context 정보만을 고려하는 것 대신에, DAFormer는 서로 다른 단계의 인코더의 feature를 통해 context 정보를 고려합니다. 이전의 고해상도에서의 추출된 추가적인 feature가 semantic segmentation을 위한 중요한 하위 단계의 개념을 제공하기 때문입니다. DAFormer 디코더의 구조는 아래 그림에서 확인할 수 있습니다.Figure 1: Rare Class Sampling, Thing-Class Feature Distance 그리고 DAFormer로 구성된 UDA framework 개요Feature fusion을 하기 전에, $1 \\times 1$ convolution을 통해 각 $F_i$를 같은 수의 채널 $C_e$로 임베딩하고 $F_1$의 크기로 feature를 upsampling 한 뒤 이를 concatenation 하였습니다. Context-aware feature fusion의 경우, ASPP7와 유사하지만 global average pooling 없이 서로 다른 dilation rate를 가지는 multiple parallel $3 \\times 3$ depthwise separable convolution과 $1 \\times 1$ convolution을 사용합니다. ASPP와는 달리, 이를 bottleneck feature $F_4$에만 적용하지 않고 모든 stacked multi-level feature를 융합하는데 적용하였습니다. Depthwise separable convolution은 일반적인 convolution보다 더 적은 수의 parameter를 가지고 있어 source domain에 대해 과적합을 줄일 수 있다는 장점이 있습니다.Training Strategies for UDARare Class Sampling(RCS)저자들은 source dataset에서 희귀 class에 대한 UDA 성능이 여러 실험에서 유의미하게 다르다는 것을 관찰했습니다. Data 샘플링 순서가 랜덤 시드에 의존하기 때문에, 이러한 class가 훈련 중 서로 다른 시점에서 학습되거나 혹은 학습되지 않을 수 있습니다. 특정 class가 훈련 중에 늦게 배울수록, 훈련이 끝날 때 그것에 대한 성능은 더 나빠집니다. 이러한 이유로 저자들은 희귀 class가 포함된 관련 샘플이 무작위성으로 인해 훈련 후반에만 나타나는 경우 network는 나중에 학습을 시작하게 되고, 이는 network가 이미 자주 등장하는 class에 대한 강한 편향을 학습하여 매우 적은 샘플로 새로운 개념(class)을 ‘재학습’하기 어려울 것이라고 생각했습니다. 이러한 점은 ST의 teacher network의 확증 편향에 따라 더욱 강화됩니다.이 문제를 해결하기 위해 저자들은 Rare Class Sampling(RCS)를 제안합니다. 이 방법은 희귀 class를 더 일찍 그리고 잘 학습하기 위해 source domain에서 더 자주 샘플링하는 방법입니다. Source dataset에서 각 class $c$의 빈도 $f_c$는 각 class $c$에 대한 pixel의 개수에 기반하여 계산할 수 있습니다.\\[f_c=\\frac{\\sum^{N_S}_{i=1}\\sum^{H \\times W}_{j=1}[y_S^{(i,j,c)}]}{N_S \\sdot H \\sdot W}\\]어떤 class $c$에 대한 샘플링 확률 $P(c)$은 그것의 빈도 $f_c$에 대한 식으로 정의됩니다.\\[P(c)=\\frac{e^{(1-f_c)/T}}{\\sum^C_{c^\\prime = 1}e^{(1-f_{c^\\prime})/T}}\\]그러므로, 더 작은 빈도를 가지는 class는 더 높은 샘플링 확률을 가지게 됩니다. Temperture $T$는 분포의 smoothness를 제어합니다. $T$가 높을수록 uniform(균일) 분포에 가깝고, $T$가 낮을수록 작은 $f_c$을 가지는 희귀 class에 집중합니다. 각 source 샘플의 경우, 확률 분포 $c \\sim P$으로부터 class를 샘플링하고 이러한 class를 포함하는 data의 부분집합에 대한 균일 분포 $x_S \\sim \\mathrm{uniform}(\\mathcal X_{S,c})$으로부터 이미지를 샘플링합니다. 이 방식은 희귀 class를 포함하는 이미지를 오버샘플링할 수 있습니다. 희귀 class(작은 $f_c$)는 일반적으로 단일 이미지에서 여러 흔한 class(큰 $f_c$)와 함께 발생하므로, re-sampling된 class간의 균형을 맞추기 위해서는 흔한 class보다 희귀 class를 더 자주($P(c_{rare}) \\gt P(c_{common})$) 샘플링하는 것이 좋습니다. 예를 들어, 흔한 class인 도로는 버스, 기차 또는 오토바이와 같은 희귀 class와 같이 나타나므로 이미 이러한 희귀 class로 샘플링할 때 이미 같이 다뤄지게 됩니다. Temperature $T$는 작은 $f_c$와 중간 $f_c$에 대해서 re-sampling된 class의 pixel의 수가 균형이 맞도록 선택됩니다.Thing-Class ImageNet Feature Distance(FD)일반적으로, semantic segmentation 모델 $g_\\theta$는 의미있는 일반적인 feature로부터 학습을 시작하기 위해 ImageNet 분류를 위해 pretrain된 weight로 초기화합니다. ImageNet에는 UDA에서 종종 구별하기 어려운 기차나 버스 같이 high-level semantic class에 대한 일부 실제 이미지도 포함되어 있다는 점을 고려하면, 저자들은 ImageNet feature가 일반적인 pretrain의 이점을 넘어 유용한 가이드를 제공할 수 있다고 생각했습니다. 특히, DAFormer는 훈련의 시작에는 몇몇 구별하기 어려운 class를 잘 구분하지만 훈련이 진행되면서 나중에는 해당 class를 잘 구분하지 못하는 문제를 관찰했습니다. 따라서, pretrain을 통한 ImageNet의 유용한 feature가 $L_S$에 의해 손상되고 모델이 synthetic source data에 과적합되었다고 볼 수 있습니다.이러한 문제를 막기 위해, semantic segmentation UDA 모델 $g_{\\theta}$의 bottleneck feature $F_\\theta$와 ImageNet 모델의 bottleneck feature $F_{ImageNet}$간의 Feature Distance(FD)에 기반하여 모델을 규제합니다.\\[d(i,j) = \\Vert F_{ImageNet}(x^{(i)}_S)^{(j)}-F_\\theta(x^{(i)}_S)^{(j)}\\Vert_2\\]그러나, ImageNet 모델은 주로 사물 class(자동차나 얼룩말처럼 윤곽이 뚜렷한 물체)에 대해 학습됩니다. 그러므로, FD loss는 이진 마스크 $M_{things}$를 통해 사물 class $C_{things}$가 포함된 이미지 영역에 대해서만 계산되어야 합니다.\\[\\mathcal{L}^{(i)}_{FD}=\\frac{\\sum^{H_F \\times W_F}_{j=1}d^{(i, j)}\\sdot M^{(i, j)}_{things}}{\\sum_jM^{(i, j)}_{things}}\\]여기서 마스크는 downscaled label $y_{S,small}$로부터 얻어집니다.\\[M^{(i, j)}_{things}=\\sum^C_{c^\\prime=1} y^{i,j,c^\\prime}_{S, small}\\sdot [c^\\prime \\in \\mathcal{C}_{things}]\\]Label을 bottleneck feature 크기로 다운샘플링하기 위해서, $\\frac{H}{H_F} \\times \\frac{W}{W_F}$크기의 patch를 통해 average pooling을 class 채널에 적용하고 class는 일정 비율 $r$을 초과할 때 유지됩니다.\\[y^c_{S,small}=[\\mathrm{AvgPool}(y^c_S,H/H_F,W/W_F)\\gt r]\\]이것은 이미지에서 주된 사물 class에 대한 bottleneck feature pixel만 feature distance에 고려되도록 합니다.전체적인 UDA loss $\\mathcal{L}$ 은 제시된 loss 요소들의 weighted sum입니다.\\[\\mathcal L= \\mathcal L_S + \\mathcal L_T + \\lambda_{FD} \\mathcal L_{FD}\\]Learning Rate Warmup for UDA훈련 시작시 선형적으로 학습률을 warmup하는 것은 CNN과 transformer 둘 다에서 성공적으로 사용되어져 왔습니다. 왜냐하면, 훈련 시작시 adaptive 학습률의 큰 분산은 gradient 분포를 왜곡하는 것을 방지하여 network의 일반화능력을 향상시키기 때문입니다.8 저자들은 이러한 학습률 warmup을 UDA에 새롭게 도입하였습니다. 저자들은 ImageNet에 대해 pretrain된 feature를 왜곡하는 것은 실제 domain에 대한 유용한 가이드를 잃는 것이기 때문에 UDA에 특히 중요함을 상정합니다. Iteration $t_{warm}$까지의 warmup 기간 동안, iteration $t$에서의 학습속도는 $\\eta_t =\\eta_{base} \\sdot t/t_{warm}$으로 설정됩니다.Conclusion이 논문은 transformer encoder와 context-aware fusion decoder를 기반으로 하여 UDA에 적합한 architecture를 제안했습니다. 이 architecture는 UDA에 대하여 transformer의 가능성을 보여줬습니다. 추가적으로 학습의 안정화와 규제를 위한, 더욱이 DAFormer의 성능을 향상시키는 3가지의 학습 기법을 제안했습니다. 전체적으로, DAFormer는 UDA에서 큰 성능 향상을 보여줬습니다. 여기서 제시된 DAFormer는 이후의 논문인 HRDA, MIC에서도 사용되는 만큼 향후 연구를 이해하는데 필수적인 논문이라 생각됩니다.Footnote Wikipedia contributors. (2022, October 24). Iverson bracket. Wikipedia. https://en.wikipedia.org/wiki/Iverson_bracket &#8617; Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Adv. Neural Inform. Process. Syst., pages 1195–1204, 2017. &#8617; Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, and Lennart Svensson. DACS: Domain Adaptation via Crossdomain Mixed Sampling. In IEEE Winter Conf. on Applications of Comput. Vis., pages 1379–1389, 2021. &#8617; Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. In Adv. Neural Inform. Process. Syst., 2021. &#8617; &#8617;2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020. &#8617; Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Int. Conf. Comput. Vis., pages 568–578, 2021. &#8617; Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Eur. Conf. Comput. Vis., pages 801–818, 2018. &#8617; Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In Int. Conf. Learn. Represent., 2019. &#8617; " }, { "title": "Sequential feature selection", "url": "/posts/Sequential-feature-selection/", "categories": "Electronic Engeering, Pattern Recognition", "tags": "Dimensionality reduction", "date": "2022-11-15 04:22:00 +0900", "snippet": "Feature subset selection(FSS)Definition특징 집합 $X=\\lbrace x_i|i=1\\dots N\\rbrace$가 주어졌을 때, 이상적으로 $P(correct)$인 목적함수 $J(Y)$를 최대화하는 $M \\lt N$인 부분집합 $Y_M$을 찾는 것Necessary 특징을 얻기가 비싼 경우입니다. 테스트 환경에서는 많은 센서를 사용할 수 있지만, 실제 제품에는 많은 센서를 사용하지 못하는 경우가 많습니다. 분류기로부터 어떠한 의미있는 법칙을 찾을 경우입니다. 특징을 투영하게 되면, 측정된 특징의 정보가 사라지는 단점이 있습니다. 특징이 숫자가 아닐 수 있습니다. 더 적은 특징을 사용하면 모델의 파라미터 수를 줄일 수 있습니다. 이는 일반화 능력을 향상시킬 수 있고 모델의 복잡도를 감소시킵니다. FSS는 여러 가능한 특징 집합에서 선택하기 위한 탐색 전략이 필요합니다. 또한, 선택된 집합을 평가하기 위한 목적 함수가 필요합니다.Search strategy특징 집합에 대해 전부 평가하려면 고정된 $M$에 대해 ${N \\choose M}$ 조합을 평가해야 하며, 만약 최적의 $M$이 있다고 하면 $2^N$의 조합을 평가해야 합니다. 이 조합의 수는 실행불가능합니다. 예를 들어 20개의 특징 중 10개를 전부 평가하려면 184,756개의 특징 집합을 평가해야 합니다. 100개 중 10개를 전부 평가하려면 $10^{13}$개의 특징 집합을 평가해야 합니다. 그러므로 탐색 전략을 통해 모든 가능한 특징 조합의 공간에서 FSS 과정에 대한 전략를 세워 탐색해야 합니다.Objective function목적 함수를 통해 후보 집합이 얼마나 좋은지에 대해 평가합니다. 이 평가를 활용하여 탐색 전략에 맞춰 새로운 후보 집합을 선택할지 결정합니다.Objective functions are divided in two groups Filters: 클래스간의 거리, 통계쩍 의존성 또는 정보-이론적 척도등의 정보 콘텐츠를 활용하여 특징 집합을 평가합니다. Wrappers: 분류기를 사용하여 통계적 리셈플링 또는 교차 검증을 통한 테스트 데이터에 대한 예측 정확도를 활용하여 부분집합을 평가합니다.Figure 1: Filter와 wrapper의 FSSFilter typesDistance or separability measures이 방법은 아래와 같은 지표를 사용하여 클래스 분리가능성을 측정합니다. 클래스간의 거리: 유클리디안, 마할라노비스 등 $S^{-1}_WS_B$의 판별식(LDA 고유값들)Correlation and information-theoretic measures이 방법은 좋은 특징 집합이 클래스와 높은 상관관계지며 서로 간에는 상관관계가 없다는 가진다는 이론적 근거를 기반으로 합니다.Linear relation measures변수간의 선형 관계는 상관계수를 사용하여 측정될 수 있습니다.\\[J(Y_M)=\\frac{\\sum^M_{i=1}\\rho_{ic}}{\\sum^M_{i=1}\\sum^M_{j=i+1}\\rho_{ij}}\\]$\\rho_{ic}$는 특징 $i$와 클래스간의 상관계수이며 $\\rho_{ij}$는 특징 $i$와 $j$의 상관계수를 의미합니다.Non-linear relation measures상관관계는 선형 의존성에 대해서만 측정이 가능합니다. 더 강력한 척도는 상호의존정보 $I(Y_k;C)$입니다.\\[J(Y_M)=I(Y_M;C)=H(C)-H(C|Y_M)=\\sum^C_{c=1}\\int_{Y_M}p(Y_m, \\omega_c)\\log\\frac{p(Y_m,\\omega_c)}{p(Y_M)P(\\omega_c)}dx\\]특징 벡터와 클래스 레이블 사이의 상호의존정보 $I(Y_M;C)$는 특징 벡터 $H(C\\vert Y_M)$를 통해 클래스의 불확실성(entropy)이 감소하는 양에 대한 척도입니다. 여기서 $H(\\sdot)$는 엔트로피 함수입니다. 상호의존정보는 다변량 밀도함수 $p(Y_M)$과 $p(Y_M, \\omega_c)$의 계산이 필요합니다. 이것은 높은 차원 공간 때문에 계산하기 어렵습니다. 실제로는, 상호의존정보는 아래 식과 같이 휴리스틱하게 바꿔 사용합니다.\\[J(Y_M)=\\sum^M_{m=1} I(x_{i_m};C)-\\beta\\sum^M_{m=1}\\sum^M_{n=m+1} I(x_{i_m};x_{i_n})\\]Filter vs. WrappersFilters 빠른 실행 속도(+) 일반적으로 반복적이지 않은 계산을 하며 이는 wrapper보다 훨씬 빠릅니다. 일반성(+) 특정한 분류기에 기반하지 않으므로, 여러 형태의 분류기를 사용할 수 있습니다. 큰 부분집합을 선택하려는 경향성(-) 목적함수가 단조함수이기 때문에 필터는 전체 특징 집합을 선택하려는 경향을 보입니다. 이에 따라 임의의 특징 개수를 지정하여야 합니다.Wrappers 정확성(+) Wrapper는 filter보다 일잔적으로 저 좋은 인식률을 보여줍니다. 왜냐하면 wrapper는 분류기와 데이터 세트간의 특정한 상호 작용에 의해 맞추어져 있기 때문입니다. 일반화하기 위한 능력(+) 일반적으로 예측 정확도에 대해 교차 검증을 사용하기 때문에 wrapper는 과적합을 피라기 위한 메커니즘이 있습니다. 느린 실행 속도(-) Wrapper는 각 특징 집합마다 분류기를 학습시켜야하므로 계산을 많이 요구하며 이는 실행하기 어려울 수 있습니다. 일반성의 부족(-) 평가에 사용된 분류기에 편향되어 있기 때문에 이 방법은 일반성이 부족하다. 최적의 특징 집합은 고려중인 분류기에 특정될 것입니다.Naïve sequential feature selection간단히 각각의 특징을 개별적으로 평가하고 잘하는 순서대로 $M$개를 선택하는 나이브한 방법을 고려해 봅시다. 불행히도, 이 방법은 잘 동작하지 않습니다. 왜냐하면, 특징간의 의존관계를 고려하지 않았기 때문입니다. 아래의 예시를 통해 확인하여 봅시다.ExampleFigure 2: 왼쪽: $X_1, X_2$에서의 각 클래스 분포, 오른쪽: $X_3, X_4$에서의 각 클래스 분포위의 Figure 2는 4차원에서의 5가지 클래스를 분류하는 문제를 보여 줍니다. 합리적인 목적 함수는 좋은 특징의 정도를 $J(x_1) \\gt J(x_2) \\approx J(x_3) \\gt J(x_4)$의 순서대로 순위를 매길 것 입니다. $x_1$은 가장 좋은 특징: $\\omega_1, \\omega_2, \\omega_3$와 $\\lbrace\\omega_4, \\omega_5\\rbrace$를 구분할 수 있습니다. $x_2$와 $x_3$는 동일: 클래스를 3개의 그룹으로 분리할 수 있습니다. $x_4$는 가장 나쁜 특징: $\\omega_4$와 $\\omega_5$ 밖에 구분할 수 없습니다.최적의 특징 집합은 $\\lbrace x_1,x_4\\rbrace$로 볼 수 있습니다. 왜냐하면 $x_4$는 단지 $x_1$이 필요로 하는 정보인 클래스 $\\omega_4$와 $\\omega_5$를 구별하는 정보를 제공하기 때문입니다. 만약, 개별적인 점수 $J(x_k)$에 대해서만 특징을 고른다면, 확실히 $x_1$과 $x_2$ 또는 $x_3$ 중 하나를 선택할 것입니다. 하지만, 이는 $\\omega_4$와 $\\omega_5$를 구별할 수 없습니다. 따라서, 이러한 나이브한 전략은 실패합니다. 왜냐하면, 특징간의 상호보완적인 정보에 대해 고려하지 않았기 때문입니다.Sequential forward selection(SFS)SFS는 가장 간단한 그리디 탐색 알고리즘입니다. 공집합에서부터 시작하며 $J(Y_k+x^+)$를 최대화하도록 연속적으로 특징 $x^+$를 추가합니다. 알고리즘 탐색 과정 Figure 3: SFS 알고리즘 Figure 4: SFS 탐색 과정 SFS는 최적의 특징 집합이 작을수록 유리합니다. 전체 특징 집합에 가까워질수록 SFS에 의하여 탐색되어야 되는 지역은 더 좁아집니다. 왜냐하면 대부분의 특징이 이미 선택되었기 때문입니다. 검색 공간이 계란형인 이유는 전제 특징집합 혹은 공집합쪽으로 갈수록 더 적은 상태가 있기 때문입니다. SFS의 주된 단점은 추가된 이전의 특징을 제거할 수 없는 단점이 있습니다.Sequential backward selection(SBS)SBS는 SFS의 역방향 구현입니다. 전체 특징 집합에서부터 시작하며, 목적함수 $J(Y-x^-)$의 값의 감소가 최소가 되도록 특징 $x^-$를 연속적으로 제거합니다. 특징을 제거하는 것이 목적함수의 값을 증가시킬수도 있습니다. 이러한 점 때문에 목적 함수는 비단조적(non-monotonic)일 수 있습니다. 알고리즘 탐색 과정 Figure 5: SBS 알고리즘 Figure 6: SBS 탐색 과정 SBS는 최적의 특징 집합이 클수록 유리합니다. 이 방법의 한계는 버린 특징을 다시 추가할 수 없다는 점입니다.Plus-L minus-R selection(LRS)LRS는 SFS와 SBS의 일반화된 구현입니다. 만약, $L\\gt R$이라면 LRS는 공집합에서 시작하고 $L$번 반복하여 특징을 추가하고 $R$번 특징을 제거합니다. 만약, $L\\lt R$이라면 LRS는 전체 특징 집합에서 시작하고 $R$번 반복하여 특징을 제거하고 $L$번 특징을 추가합니다.LRS는 SFS와 SBS가 선택을 되돌릴 수 없는 약점을 보완하기 위한 방법입니다. 이 방법의 한계는 최적의 $L$과 $R$을 예측하는 것이 어렵다는 것입니다. 알고리즘 탐색 과정 Figure 7: LRS 알고리즘 Figure 8: LRS 탐색 과정 Bidirectional Search(BDS)BDS는 SFS와 SBS의 병렬화된 구현입니다. SFS는 공집합에서부터 시작하고, SBS는 전체 특징 집합에서부터 시작합니다. SFS와 SBS가 같은 solution에 수렴되도록 보장되어야 합니다. 이는 아래 두가지 조건을 지키면 보장할 수 있습니다. SFS에 의해 선택된 특징은 SBS에 의해 제거되어서는 안됩니다. SBS에 의해 제거된 특징은 SFS에 의해 선택되어서는 안됩니다. 알고리즘 탐색 과정 Figure 9: BDS 알고리즘 Figure 10: BDS 탐색 과정 Sequential floating selection(SFFS and SFBS)LRS의 백트래킹 능력을 좀 더 확장한 방법입니다. $L$과 $R$을 고정시키지 않고, 데이터를 통해 결정하도록 합니다. 부분집합의 차원이 검색 중에 유동적으로 증가 및 감소합니다.There are two floating methodsSequential floating forward selection(SFFS)는 공집합으로부터 시작합니다. 특징을 추가하는 단계 이후, SFFS는 가능한 목적 함수가 증가하는 한 특징을 제거하는 단계를 수행합니다. Sequeuntial floating backward selection(SFBS)는 전체 특징 집합에서 시작합니다. 특징을 제거하는 단계 이후, SFBS는 가능한 목적 함수가 증가하는 한 특징을 추가하는 단계를 수행합니다. 알고리즘 탐색 과정 Figure 11: SFFS 알고리즘 Figure 12: SFFS와 SFBS 탐색 과정 제거된 특징은 기록하며 처리해야 무한 루프를 피할 수 있습니다.Reference 홍광석, “패턴인식론(ECE5302)” (대학강의, 성균관대학교, 2022년 가을학기)" }, { "title": "Linear discriminant anlaysis", "url": "/posts/Linear-discriminants-analysis/", "categories": "Electronic Engeering, Pattern Recognition", "tags": "Dimensionality reduction, LDA", "date": "2022-11-13 01:32:00 +0900", "snippet": "Linear discriminant analysis, two classes먼저, 클래스가 두 개인 경우에 한하여 생각합시다.ObjectiveLinear discriminant analysis(LDA)는 클래스의 판별정보를 보존하면서 차원을 감소시키는 방법입니다. 먼저, 클래스 $\\omega_1$에 속하는 $N_1$, $\\omega_2$에 속하는 $N_2$개의 $D$-차원의 샘플 $\\lbrace{x^{(1},x^{(2}, \\dots x^{(N}}\\rbrace$이 있다고 가정합시다. 이 샘플 $x$를 어떠한 선으로 투영시켜 스칼라 $y$를 얻습니다.\\[y=w^Tx\\]이제 투영된 스칼라 $y$를 쉽게 분리할 수 있도록 투영하는 투영 벡터 $w$를 찾으면 됩니다.Figure 1: 빨간색 클래스와 파란색 클래스를 적절한 선에 투영시켜 클래스를 구분좋은 투영 벡터를 찾기 위해서는, 스칼라 $y$가 잘 분리되도록 척도를 정의해야 합니다. $x$ 공간과 $y$ 공간에서 각 클래스의 평균 벡터는 아래와 같이 정의할 수 있습니다.\\[\\textstyle\\mu_i=\\frac{1}{N_i}\\sum_{x \\in w_i}x \\space and \\space \\tilde{\\mu}_i=\\frac{1}{N_i}\\sum_{y \\in w_i}y=\\frac{1}{N_i}\\sum_{x \\in w_i}w^Tx=w^T\\mu_i\\]LDA에서는 잘 분리하기 위한 목적함수로써 투영된 평균간의 거리를 사용합니다.\\[J(w)=\\vert\\tilde{\\mu}_1-\\tilde{\\mu}_2\\vert=\\vert w^T(\\mu_1-\\mu_2)\\vert\\]Fisher’s solutionFisher는 within-class scatter 척도에 의하여 정규화된 평균간의 차이를 최대화하는 것을 제안하였습니다. 여기서 scatter는 각 클래스에 대한 분산과 유사한 개념입니다.\\[\\tilde{s}^2_i=\\textstyle\\sum_{y \\in \\omega_i}(y-\\tilde{\\mu}_i)^2\\]이를 통해, Fisher linear discriminant는 아래와 같이 정의됩니다.\\[J(w)=\\frac{\\vert\\tilde{\\mu}_1-\\tilde{\\mu}_2\\vert^2}{\\tilde{s}^2_1+\\tilde{s}^2_2}\\]이는 같은 클래스끼리는 서로 가까이 투영되게 하되 가능한 투영된 평균은 서로 멀리 투영되도록 하는 $w$를 찾는 것입니다.Figure 2: 좌측의 직선에 각 클래스를 투영Find the optimum먼저, $x$ 공간에서의 scatter를 계산합니다.\\[S_i^2=\\textstyle\\sum_{x \\in \\omega_i}(x-\\mu_i)(x-\\mu_i)^T \\\\S_1^2+S_2^2=S_W\\] $S_W$는 within-class scatter라고 불립니다.투영된 y 공간에서의 scatter를 특징 공간 x의 scatter 행렬의 형태로 표현합니다.\\[\\begin{align*}\\tilde{s}^2_i&amp;=\\textstyle\\sum_{y \\in \\omega_i}(y-\\tilde{\\mu}_i)^2 = \\sum_{x \\in \\omega_i}(w^Tx-w^T\\mu_i)^2 \\\\ &amp;=\\textstyle\\sum_{x \\in \\omega_i}w^T(x-\\mu_i)(x-\\mu_i)^Tw=w^TS^2_iw\\end{align*}\\]\\[\\boxed{\\tilde{s}^2_1+\\tilde{s}^2_2=w^TS_Ww}\\]이와 동일하게, 투영된 평균 간의 차이 또한 투영되기 전의 특징 공간에서의 평균으로 표현할 수 있습니다.\\[(\\tilde{\\mu}_1-\\tilde{\\mu}_2)^2=(w^T\\mu_1-w^T\\mu_2)^2 = w^T(\\mu_1-\\mu_2)(\\mu_1-\\mu_2)^Tw=\\boxed{w^TS_Bw}\\] $S_B$는 between-class scatter라고 불립니다. $S_B$는 두 벡터의 외적이며, 이것의 랭크는 최대 1이라는 점에 주목하세요.최종적으로, $S_W$와 $S_B$으로 Fisher criterion을 표현할 수 있습니다.\\[J(w)=\\frac{w^TS_Bw}{w^TS_Ww}\\]$J(w)$의 최대값을 찾기 위해 미분하고 0이 되는 지점을 계산합니다.\\[\\begin{aligned}\\frac{d}{dw}[J(w)] = \\frac{d}{dw}\\Bigg\\lbrack\\frac{w^TS_Bw}{w^TS_Ww}\\Bigg\\rbrack &amp;= 0 \\Rightarrow \\\\\\lbrack w^TS_Ww\\rbrack \\frac{d[w^TS_Bw]}{dw}-\\lbrack w^TS_Bw\\rbrack \\frac{d[w^TS_Ww]}{dw} &amp;=0 \\Rightarrow \\\\\\lbrack w^TS_Ww\\rbrack 2S_Bw-\\lbrack w^TS_Bw\\rbrack 2S_Ww &amp;=0\\end{aligned}\\]$w^TS_Ww$로 나눕니다.\\[\\begin{aligned}\\Bigg\\lbrack\\frac{w^TS_Ww}{w^TS_Ww}\\Bigg\\rbrack S_Bw-\\Bigg\\lbrack\\frac{w^TS_Bw}{w^TS_Ww}\\Bigg\\rbrack S_Ww&amp; =0 \\Rightarrow \\\\S_Bw-JS_Ww &amp;=0 \\Rightarrow \\\\S^{-1}_WS_Bw-Jw&amp; =0\\end{aligned}\\]최종적으로는 고유값을 구하는 문제($S^{-1}_WS_Bw=Jw$)와 동일하게 됩니다.\\[\\boxed{w^*=\\arg \\max \\Bigg\\lbrack\\frac{w^TS_Bw}{w^TS_Ww}\\Bigg\\rbrack=S^{-1}_W(\\mu_1-\\mu_2)}\\]정확히는 판별식은 아니고, 투영 벡터를 찾는 방법이지만 이것은 Fisher's linear discriminant(1936)로 알려져 있습니다.Linear discriminant analysis, C classes이제, C개 클래스를 분류하는 문제로 확장시켜 봅시다. 기존에서는 하나의 투영으로 두 개의 클래스를 분류했다면, 이제는 $(C-1)$개의 투영으로 C개의 클래스를 분류할 수 있습니다. 투영 벡터 $w_i$는 이제 투영 행렬 $W=[w_1|w_2|\\dots|w_{C-1}]$로 확장됩니다.\\[y_i=w^T_ix \\Rightarrow y = W^Tx\\]Figure 3: 각 분포의 평균과 scatterDerivationwithin-class scatter는 아래처럼 일반화 될 수 있습니다.\\[S_W=\\textstyle\\sum^C_{i=1}S_i^2\\] 이때 $S_i^2=\\sum_{x \\in \\omega_i}(x-\\mu_i)(x-\\mu_i)^T$이고 $\\mu_i=\\frac{1}{N_i}\\sum_{x \\in \\omega_i} x$입니다.그리고 between-class scatter는 아래처럼 됩니다.\\[S_B=\\textstyle\\sum^C_{i=1}N_i\\lparen\\mu_i-\\mu\\rparen\\lparen\\mu_i-\\mu\\rparen^T\\] 이때 $\\mu=\\frac{1}{N}\\sum_{\\forall x}x=\\frac{1}{N}\\sum^C_{i=1}N_i\\mu_i$입니다.행렬 $S_T=S_B+S_W$는 total scatter라고 불립니다. 유사하게, 투영된 샘플에 대해 평균 벡터와 scatter 행렬을 구합니다.\\[\\begin{aligned}\\tilde{\\mu}_i&amp;=\\frac{1}{N_i}\\textstyle\\sum_{y \\in \\omega_i} y&amp;\\tilde{S}_W &amp;=\\textstyle\\sum^C_{i=1}\\sum_{y \\in \\omega_i}(y-\\tilde{\\mu}_i)(y-\\tilde{\\mu}_i)^T \\\\\\tilde{\\mu} &amp;=\\frac{1}{N}\\textstyle\\sum_{\\forall y} y &amp;\\tilde{S}_B &amp;=\\textstyle\\sum^C_{i=1}N_i(\\tilde{\\mu}_i-\\tilde{\\mu})(\\tilde{\\mu}_i-\\tilde{\\mu})^T \\\\\\end{aligned}\\]클래스가 두 개인 경우에서의 미분과 마찬가지로, 아래와 같이 표현할 수 있습니다.\\[\\begin{aligned}\\tilde{S}_W&amp;=W^TS_WW \\\\\\tilde{S}_B&amp;=W^TS_BW\\end{aligned}\\]최종적인 목적함수는 분모와 분자에 판별식이 추가된 형태입니다.\\[J(W)=\\frac{|\\tilde{S}_B|}{|\\tilde{S}_W|}=\\frac{|W^TS_BW|}{|W^TS_WW|}\\]여기서 판별식이 추가된 이유는 계산된 scatter 행렬을 스칼라 값으로 바꾸기 위해서입니다. 최종적으로 최적의 $W$는 아래와 같습니다.\\[\\boxed{W^*=[w^*_1|w^*_2|\\dots w^*_{C-1}] = \\arg \\max \\frac{|W^TS_BW|}{|W^TS_WW|} \\Rightarrow(S_B-\\lambda_iS_W)w^*_i=0}\\]$S_B$는 랭크가 $\\le 1$인 $C$ 행렬들의 합이고 평균 벡터들은 $\\frac{1}{C}\\sum^C_{i=1}\\mu_i=\\mu$을 만족합니다. 그러므로 $S_B$의 랭크는 $C-1$보다 같거나 작습니다. 이는 0이 아닌 $C-1$개의 고유값을 얻을 수 있다는 것을 의미합니다. 클래스 분리성이 최대인 투영은 $S^{-1}_WS_B$의 가장 큰 고유값과 대응되는 고유 벡터입니다.LDA vs. PCAFigure 4: 가스 센서 데이터의 PCA와 LDA5 종류의 커피 냄새 분류를 위해 가스 센서로부터 데이터를 수집하여 처리한 예시입니다. PCA의 경우, 전반적으로 데이터가 넓게 분포하고 있지만 LDA는 특징을 잘 판별하도록 데이터가 분포하고 있습니다.Limitations of LDALDA produces at most C-1 feature projections분류를 위해서 더 많은 특징이 필요하다면, 추가적인 특징을 제공하기 위해서는 다른 방법을 사용해야 합니다.LDA is a parametric method분포가 unimodal 가우시안 분포를 따른다면 LDA를 사용했을 때 좋은 성능을 기대할 수 있습니다. 하지만, 이외의 형태는 잘 분류하지 못합니다.Figure 5: LDA로 구분하기 어려운 분포LDA will also fail if discriminatory information is not in the mean but in the variance of the data분포간의 평균의 차이가 적을 경우, LDA를 사용하면 잘 분류되지 않습니다.Figure 6: PCA가 변별에 유리한 분포Variants of LDANon-parametric LDA국부적인 정보와 kNN을 사용하여 $S_B$을 계산함으로써 unimodal 가우시안 추정을 하는 비모수적 LDA입니다. 즉, 클래스 안에서 클러스터를 구성하고 클러스터간 LDA를 수행한다고 볼 수 있습니다. 이 결과, 행렬 $S_B$는 full-rank이며 $(C-1)$개 이상의 특징을 추출할 수 있습니다. 이러한 방식은 데이터의 구조를 더 잘 보존할 수 있습니다.Orthonormal LDAOLDA는 Fisher criterion을 최대화하고 동시에 pair-wise 직교 정규 분포를 따르는 투영을 계산합니다. OLDA에서 사용되는 이 방법은 $S^{-1}_WS_B$의 고유값 문제와 그람-슈미트 직교화 과정의 결합으로 볼 수 있습니다. OLDA는 순차적으로 추출된 모든 특징에 직교하는 부분공간에서 Fisher criterion을 최대화하는 축을 찾습니다. OLDA는 $(C-1)$개 이상의 특징을 찾을 수 있습니다.Generalized LDAGLDA는 베이즈 리스크를 계산하는 데 사용된 것과 유사한 비용 함수를 활용하여 Fisher criterion을 일반화합니다. 비용이 높을 경우에는 높은 차원에 투영하고 비용이 낮으면 낮은 차원에 투영합니다.Multilayer perceptrons다층 퍼셉트론은 마지막 은닉층의 결과인 scatter 행렬 $Tr[S_BS^\\dagger_T]$를 최대화하여 LDA를 수행할 수 있습니다.Other dimensionality reduction methodsExploratory Projection Pursuit“관심”을 척도로 하여 이를 최대화하도록 $M$-차원(주로, $M=2,3$)으로 선형 투영하는 방법입니다. “관심”이란 다변량 정규성으로부터 떨어진 정도를 의미합니다. 이 척도는 분산을 의미하는 것은 아닙니다. 그리고 보통 scale-free 합니다. 대부분의 구현에서 affine invariant하므로 특징 간의 상관관계에 의존하지 않습니다. 다른 말로는, EPP는 가능한 많이 분리하고 각 클러스터 안에서는 최대한 밀착되도록 하는 투영을 찾는 것입니다. Fisher의 방법과 유사하지만, EPP는 클래스 라벨을 사용하지 않습니다. 일단 관심있는 투영이 발견되면, 다른 관심있는 관점을 더 쉽게 찾기 위해 그 구조를 제거합니다.Figure 7: EPP의 적용Sammon’s non-linear mapping이 방법은 점간의 거리는 최대한 보존하며 $N$-차원의 공간을 $M$-차원의 공간으로 맵핑하는 것입니다. 이는 아래의 목적함수를 최소화하는 것으로 달성할 수 있습니다.\\[E(d, d')=\\textstyle\\sum_{i \\neq j}\\frac{[d(P_i, P_j)-d(P'_i, P'_j)]^2}{d(P_i, P_j)}\\]초기에는 명시적인 맵핑을 얻을 수는 없었고 훈련 세트에서 룩업 테이블을 구현하는 것에 지나지 않았습니다만, 신경망에 기반한 구현을 통해 테스트 데이터에 대해 명시적인 맵핑을 얻을 수 있게 됐었습니다. 또한, 비용 함수를 고려할 수 있게 되었습니다.새몬 맵핑은 사회 과학에서 주로 사용되는 다변량 통계 방법 중 Multi Dimensional Scaling(MDS)와 매우 관련되어 있습니다.Reference 홍광석, “패턴인식론(ECE5302)” (대학강의, 성균관대학교, 2022년 가을학기)" }, { "title": "Symbolic link in Linux", "url": "/posts/Symbolic-link-in-Linux/", "categories": "Linux", "tags": "linux, command, symbolic link", "date": "2022-09-28 13:20:00 +0900", "snippet": "AI를 연구하기 위해 다양한 실험을 진행하다보면, 동일한 dataset에 대해서 다양한 프로젝트에서 학습하고 테스트하는 경우가 많습니다. 여러 프로젝트는 각자 독립적인 디렉토리 구조를 가지고 있습니다. 하지만, 매번 모든 폴더에 대해 데이터를 옮기거나 복사할 수는 없습니다. 이를 편리하게 진행하기 위해 다른 위치의 폴더를 원하는 위치로 연결하는 방법인 symbolic link에 대해 설명하겠습니다.Linux의 명령어를 사용하여 symbolic link를 생성 및 제거할 수 있습니다. Linux에서 사용할 수 있는 symbolic link에는 두 가지 유형이 있습니다. Hard link 기존 파일에 대한 별칭으로 이해하면 됩니다. 즉, 원본 파일의 inode1 위치를 가리킵니다. 이 때문에 사실상 원본 파일과 hard link 파일을 구별할 수 없습니다. 하나의 파일에 대해 하나 이상의 hard link를 만들 수 있습니다. 하지만, 다른 파일 시스템 또는 파티션에 저장된 디렉토리 또는 파일에 대해서는 hard link를 만들 수 없습니다. Soft link Windows의 바로 가기와 동일합니다. 다른 위치에 있는 파일이나 디렉토리를 가리키는 데 사용됩니다.일반적으로 AI 개발을 위한 서버는 연산을 담당하는 서버와 데이터를 저장하는 파일 서버로 나뉘어져 있습니다. Hard link는 다른 파일 시스템의 데이터와 디렉토리에 대해서 생성이 불가능하기 때문에 soft link를 사용하는 것을 추천합니다.Creating a symbolic link to fileln -s source_file symbolic_link위의 명령에서 source file은 연결할 Linux 시스템의 파일 이름입니다. symbolic_link는 생성할 symbolic link의 이름입니다. Hard link를 생성할 때에는 -s 옵션을 빼고 사용하면 됩니다.Removing symbolic linkunlink symbolic_linkrm symbolic_link생성된 symbolic link를 제거하기 위해서는 unlink 혹은 rm 명령을 사용하면 됩니다.Overwriting symbolic linkln -sf source_file symbolic_link동일한 이름의 symbolic link가 있는 경우에는 -f 옵션을 사용하면 덮어쓰기하여 생성할 수 있습니다.Reference Symbolic Link in LinuxFootnote Wikipedia contributors. (2022, June 14). Inode. https://en.wikipedia.org/wiki/Inode &#8617; " }, { "title": "How to Read a Paper", "url": "/posts/How-to-Read-a-Paper/", "categories": "Paper Review", "tags": "paper review, SIGCOMM", "date": "2022-09-14 13:40:00 +0900", "snippet": " Keshav, Srinivasan. “How to read a paper.” ACM SIGCOMM Computer Communication Review 37.3 (2007): 83-84.대학원 생활을 하면서 여러 논문을 읽을 기회가 생겼습니다. 문제는 논문 하나를 읽는데에 걸리는 시간이 너무나 길었습니다. 어떻게 하면 논문을 효율적으로 읽을 수 있을지 고민하였고 이 페이퍼를 찾게 되었습니다. 2007년에 나온 다소 오래된 논문이지만 저자가 제시한 방법은 꽤 그럴싸해 보입니다. 아래에 해당 페이퍼 전문에 대한 내용을 정리하였습니다.Abstract연구자들은 논문을 읽은데 많은 시간을 할애합니다. 하지만, 논문을 효과적으로 읽는 방법에 대해서는 우리는 잘 배우지 못했습니다. 이 논문은 3단계 접근 방법을 통해 실용적이고 효율적인 논문 읽기 방법에 대해 설명합니다.Introduction연구자들은 다양한 이유로 논문을 읽습니다. 대부분의 연구자들은 매년 수백시간을 논문을 읽는데 사용합니다. 논문을 효율적으로 읽는 것을 매우 중요하지만, 대부분은 그걸 수업을 통해 배우지는 않습니다. 대학원생들은 주로 시행 착오를 통해 이러한 방법을 체득합니다. 이 과정에서 많은 노력을 하고 있고 좌절에 빠지고 있습니다.수 년간 저자는 논문을 효과적으로 읽기 위해 간단한 접근 방식을 사용해왔습니다. 이 논문에서는 ‘3단계’ 접근 방식과 이 방식을 문헌 조사에서의 사용하는 법에 대해서 기술하고 있습니다.The Three-Pass ApproachThe first pass첫번째 단계는 넓은 시야에서 빠르게 논문 전체를 스캔하는 것입니다. 여기서 여러분은 앞으로 다음 단계로 나아갈지 결정하면 됩니다. 이 단계에 걸리는 시간은 5 ~ 10분정도가 적당합니다.첫번째 단계는 아래의 단계로 나뉩니다. 신중하게 논문의 제목(Title), 초록(Abstract), 소개(Introduction)를 읽습니다. 각 섹션과 하위 섹션의 제목을 읽습니다. 이 외의 정보는 모두 무시하세요. 결론(Conclusion)을 읽습니다. 참조(Reference)에서 여러분이 이미 읽은 것들이 있는지 살펴보세요.이 단계가 끝나면 여러분은 아래 5개의 질문에 답할 수 있어야 합니다. Category: 이 논문의 종류는 무엇인가? Context: 이 논문은 어떤 다른 논문과 연관되어 있는가? 어떤 이론적 기반에 근거하여 문제를 분석하고 있는가? Correctness: 논문의 가정이 유효한 가정인가? Contributions: 논문의 주요한 기여점은 무엇인가? Clarity: 논문이 잘 쓰여졌는가?이러한 정보를 활용하면, 다음 단계로 나아갈지 결정할 수 있을겁니다.이러한 점은 여러분이 논문을 쓸 때에도 활용될 수 있습니다. 최대한 간결하고 이해하기 쉬운 초록과 일관 섹션과 하위 섹션의 제목에 유의하며 작성하여야 합니다. 만약, 리뷰어가 1단계에서 잘 이해가 가지 않는다면 여러분의 논문은 1단계에서 거절(Reject)될 확률이 높습니다.The second pass2단계에서는 훨씬 더 신중히 논문을 읽어야 합니다. 하지만 증명과 같은 세부적인 내용을 무시하세요. 논문을 읽으면서 키 포인트를 적고, 여백에 관련 코멘트를 적으세요. 피겨(Figure), 도표(Diagram), 여러 다른 삽화들을 매우 신중히 보세요. 특히 그래프에 집중하세요. 각 축은 어떻게 되어 있으며 각 막대기는 무엇을 보여주는지 등을 파악하세요. 앞으로 읽을 읽지 않은 참조 논문을 표시해서 기억하세요. 이러한 방법은 논문의 배경에 대해 더 많이 배울 수 있습니다.2단계는 1시간 정도 소요됩니다. 이 단계가 끝나면 여러분은 논문의 내용을 파악했다고 볼 수 있습니다. 이제 여러분은 여러 뒷받침되는 증거를 활용하여 논문의 핵심을 요약할 수 있게 됩니다.가끔씩 2단계를 끝내고 나서도 논문에 대해 정확히 이해하지 못하는 경우가 있습니다. 이러한 원인은 논문의 주제가 여러분에게 친숙하지 않은 용어, 줄임말을 등을 비롯하여 새롭기 때문입니다. 또는, 저자가 여러분이 이해하기 어려운 증명이나 기술을 사용했을수도 있습니다. 또는, 논문이 입증되지 않은 주장을 펼피는 등 매우 잘못 쓰여졌을수도 있습니다. 마지막으로, 그냥 여러분이 피곤해서 잘 안 읽히는 것일 수도 있습니다. 이렇게되면 당신은 아래 3가지 중 하나를 선택할 수 있습니다. 논문을 옆에 두고, 이것을 안 읽어도 당신의 커리어에 아무런 문제가 없길 바라는 것 논문의 배경을 읽은 후 다시 읽는 방법 인내하며 3단계로 가는 방법The third pass논문을 완전히 이해하기 위해서는 3단계가 필요합니다. 3단계의 핵심은 논문을 virtually re-implement 하는 것입니다. 다시 말해, 저자들과 똑같은 가정을 하고 다시 논문을 재현해 보는 것입니다. 재현 후 논문과 비교해보면 쉽게 논문을 이해할 수 있을 뿐만 아니라 숨겨진 실패와 가정을 발견할 수 있을겁니다.이 단계에서는 세부적인 부분에 엄청나게 집중해야 합니다. 매 문장마다 모든 가정을 확인하고 의심해봐야 합니다. 이 단계에서 여러분은 앞으로의 작업에 대한 생각도 적어놔야 합니다.이 단계는 4 ~ 5시간정도 소요될 것입니다. 숙련된다면 1시간정도면 충분합니다. 이 단계의 마지막으로 여러분의 기억으로부터 강점과 약점을 파악하며 논문의 전체 구조를 재구성 해보세요. 특히, 여러분은 숨겨진 가정, 빠진 인용 그리고 실험적 혹은 분석적 기술의 잠재적인 문제에 대해 집어 낼 수 있어야 합니다.Doing a literature survey첫째, Google Scholar나 CiteSeer와 같은 논문 검색 엔진을 사용하세요. 키워드를 잘 선택하여 그 분야의 3 ~ 5개 정도의 최신 논문을 찾아보세요. 각 논문에 대해 감을 익히기 위해 1단계 방법을 적용해보세요. 그리고 related work 섹션을 읽으세요. 여러분은 최근 연구들의 전반적인 그림을 찾을 수 있을 것입니다. 만약, 서베이 논문을 찾았다면 해당 논문을 읽으면 됩니다.반면에, 두번째 단계에서는 공유되는 저자 이름을 찾으세요. 이 분야에서 중요한 인물일 것 입니다. 주요 논문을 다운로드하고 옆에 놔두세요. 주요 저자들의 웹사이트로 가서 최근에 그들이 어떤 것을 투고했는지 봅니다. 그들은 아마 해당 분야의 탑 컨퍼런스에 확인하기 쉬울 것입니다. 최고의 연구자들은 탑 컨퍼런스에 주로 투고합니다.세번째 단계에서는 이러한 탑 컨퍼런스 웹사이트로 가서 최근 내역을 살펴보세요. 빠르게 스캔하여 좋은 연관된 논문을 확인하세요.Experience저자는 지난 15년간 논문을 읽을 때 이러한 방식을 사용했습니다. 이러한 접근은 전체를 보기 전에 세부적인 부분에 빠지는 문제로부터 막아줬습니다. 또한, 필요성과 저자가 가지고 있는 시간에 기반하여 논문을 평가하는 시간을 조정할 수 있었습니다.아마 많은 대학원생들이 저와 비슷한 고민을 했거나 지금도 하고 있을 것입니다. 저는 이 페이퍼를 기반으로 앞으로의 논문을 읽어볼 생각입니다. 여러분도 논문 읽기에 고민이 있으시다면 Three-Pass Approach를 한번 시도해보세요." }, { "title": "Install MATLAB on Linux", "url": "/posts/Install-MATLAB-on-Linux/", "categories": "Computer Language, MATLAB", "tags": "matlab, troubleshooting, programming language, installation", "date": "2022-06-13 23:38:00 +0900", "snippet": "Installing process Linux용 MATLAB 설치 압축파일을 MathWorks에서 다운로드하세요. Matlab 폴더를 생성 후 해당 폴더 안에서 다운로드 받은 파일의 압축을 푸세요. sudo ./install를 입력하세요. System password를 입력하세요. 실행되는 프로그램의 지시에 따라 설치를 계속합니다. ❗프로그램이 실행되 않을 경우 export DISPLAY=':0.0'을 입력 후 3번부터 다시 시도해보세요.Creating a shortcut for MATLAB on the launcher 패키지를 설치합니다. sudo apt-get install matlab-support TroubleshootingNo Matlab install GUI is displayed xhost +SI:localuser:root 를 입력하세요.Failed to load module “canberra-gtk-module” 패키지를 설치합니다. apt-get install libcanberra-gtk-module GTK_PATH를 설정합니다. export GTK_PATH=/usr/lib/x86_64-linux-gnu/gtk-2.0 Can’t install any toolboxes because can’t write to the path Matlab Command Window에서 Matlab이 설치된 위치를 찾습니다. matlabroot 소유자를 변경합니다. chown -R {username}:{/path/to/matlab/root/path} Broken Korean fonts(한글 깨짐) 나눔 폰트를 설치합니다. apt install fonts-nanum fonts-nanum-coding fonts-nanum-extra Reference How do I launch MATLAB on Linux? Installer hang when installing Matlab R2021b as root on Ubuntu 20.04 canberra-gtk-module 및 pk-gtk-module 메시지 제거하기 - MATLAB &amp; Simulink - MathWorks 한국 Can’t install any toolboxes because can’t write to /usr/local/MATLAB/R2017 리눅스 민트 한글 깨짐 현상을 해결하려면 어떻게 해야 하나요." } ]

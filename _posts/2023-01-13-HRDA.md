---
title: "HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation"
date: 2023-01-13 12:53
category: [Paper Review]
tags: [paper review, ECCV]
math: true
published: true
img_path: "/assets/img/posts/2023-01-13-HRDA"
---

> Hoyer, L., Dai, D., & Van Gool, L. (2022). HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation. Proceedings of the European Conference on Computer Vision (ECCV).

# Introduction
이 연구는 UDA에 관한 연구입니다. Target dataset에 대해 annotation을 하지 않기 위해, network는 이미 존재하거나 annotation 하기 쉬운 source dataset을 통해 훈련됩니다. 그러나, neural network은 domain shift에 매우 민감합니다. 이러한 문제는 UDA에서  source data에 대해 훈련된 network를 unlabeld target image에 adapt하여 해결합니다.

UDA 방법은 일반적인 supervised learning 보다 더 많은 GPU 메모리가 필요합니다. 왜냐하면, UDA는 주로 여러 domain의 이미지, 추가적인 network와 GPU 메모리에 상당한 영향을 주는 추가적인 loss를 필요로 합니다. 그러므로, 대부분의 UDA semantic segmentation 방법은 GPU 메모리 제약 때문에서 이미지를 downscaling하여 사용합니다. 그에 반해 대부분의 supervised 방법은 downscaling 없이 그대로 사용합니다.

저해상도 입력에서의 예측은 종종 먼 거리의 신호동과 같은 작은 객체를 인식하는 데 실패하고 먼 거리의 보행자의 팔다리와 같은 fine segmentation detail을 유지하지 못합니다. 그러나 전체 고해상도 image로 naively 학습하는 것은 GPU 메모리 요구량에 상당한 영향을 주기 때문에 사용하기 어렵습니다. 일반적인 해결책은 image의 random crop을 통해 훈련시키는 것입니다. HR crop을 이용하여 훈련하면 작은 객체을 adapt하고 segmentation detail을 유지할 수 있습니다. 이 방법은 UDA의 경우에 context information과 scene layout이 종종 domain-robust 해야 하기 때문에 큰 단점으로 작용합니다. HR 입력은 작은 객체를 adapt하는데 필수적인 반면, 좁은 보도와 같은 크기 큰 객체 영역을  adapt하기에는 단점이 있습니다. HR은 이러한 영역에 대해 종종 너무 detail하며, 특정 domain에만 국한된 feature를 가지고 있습니다. 이러한 점은 UDA에서 불리하게 작용합니다. LR 입력은 이러한 feature를 감추고, domain에 걸쳐 넓은 영역을 인식하기 위한 충분한 detail을 제공합니다.

어느정도 감당 가능한 GPU 메모리 공간을 유지하면서 두 접근 방식의 강점을 효율적으로 결합하기 위해, 저자들은 *HRDA*를 제안합니다. 먼저, HRDA는 특정 domain에 국한된 HR texture를 혼동하지 않도록 큰 객체에 대해 adapt시키기 위해 큰 크기의 LR *context crop*을 사용합니다. 또한, HR detail은 long-range dependency에 대해 중요하지 않다고 생각하기 때문에 long-range context를 학습시키기 위해서도 LR context crop을 사용합니다. 둘째, HRDA는 작은 객체를 adapt하고 segementation detail을 유지하기 위해 context crop 내의 영역에서 작은 HR *detail crop*을 사용합니다. 또한, long-range context information은 segmentation detail을 학습하는데 부가적인 역할만을 한다고 생각하기 때문에 segmentation detail을 보존하기 위해 HR detail crop을 사용합니다. 이러한 방식을 통해, GPU 메모리 사용량은 크게 줄이면서 큰 crop 크기와 고해상도의 이점을 유지합니다. 이미지의 내용에 따라 LR context crop과 HR detail crop간의 중요도가 달라진다는 점을 고려하기 위해, HRDA는 input-dependent scale attention을 사용하여 이 둘을 합칩니다. Attention은 LR 및 HR의 예측이 얼마나 신뢰할 수 있는지에 대해 결정하는 법을 학습합니다. Supervised learning을 위한 이전 multi-resolution framework는 전체 LR 및 HR 이미지를 사용하기 때문에서, GPU 메모리 제약으로 인해 UDA에서 naive하게 적용될 수 없습니다. 또한, HRDA을 target domain으로 adapt하기 위해 여러 해상도가 합쳐진 pseudo-label을 통해 훈련을 할 수 있습니다. 더욱이, 서로 다른 context에 대해 detail pseudo-label의 robustness를 증가시키기 위해 pseudo-label은 overalpping sliding window mechanism을 사용하여 생성됩니다.

# Method

## Context and Detail Crop

GPU 메모리 제한사항 때문에, UDA에서 요구되는 여러 domain에서 전체 크기의 고해상도/여러 해상도의 이미지 입력과 추가적인 network 와 loss를 통해 SOTA UDA 모델을 학습하는 것은 불가능합니다. 그러므로 대부분의 이전 연구에서는 LR 입력만을 사용합니다. 그러나, HR 입력은 작은 객체를 인식하고 fine segmentation 경계를 생성하는데 중요합니다. HR 입력을 이용하기 위해선, random cropping이 가능한 대안이 될 수 있습니다. 하지만, random cropping은 길 위에 차가 있는 장면이나 자전거 위에 사람이 타고 있는 장면과 같이 context relation은 종종 doamin-invariant하기 때문에 UDA에서 중요한 scene layout과 long-range dependency에 대한 context-aware semantic segmentation을 학습하는데 제한이 될 수 있습니다. 고해상도와 long-range context를 모두 사용하여 학습하기 위해서, 저자들은 서로 다른 해상도에서 대해서 서로 다른 crop 크기를 결합하는 것을 제안합니다. 예를 들어, 아래 그림과 같이 LR context crop과 작은 HR detail crop을 결합합니다.

![Figure x: HRDA](HRDA.png)
*Figure 1: (a) 저해상도(LR, low-resolution)와 고해상도(HR, high-resolution) detail crop을 이용한 multi-resolution 훈련. Detail crop의 prediction은 학습된 scale attention을 통해 context prediction에서 detail crop이 잘린 부위와 대응되는 영역과 합쳐짐. (b) Pseudo-label 생성의 경우, 여러 detail crop을 통해 전체 context crop을 생성하기 위해 overlapping slide inference를 사용하여 생성됨. Pseudo-label은 HR pred. $\hat y_{c,HR}^T$와 LR pred. $\hat y_c^T$를 (a)와 유사하게 full attention을 통해 합침.*

Context crop은 long-range context를 학습시키기 위한 large crop을 제공하는 것이 목적입니다. Detail crop은 HR을 통해 작은 객체를 인식하고 fine segmentation detail을 생성하는 것이 목적입니다. Model validation동안에는 전체 이미지에 대해 segmentation을 하기 위해 overlapping sliding window inference가 사용되었습니다.

Context crop $x_c \in \mathbb R^{h_c \times w_c \times 3}$는 원래 HR 이미지 $x_{HR} \in \mathbb R^{H \times W \times 3}$에 대한 cropping과 $s \ge 1$인 factor를 통해 bilinear downsampling을 통해 얻어집니다.

$$
x_{c, HR}=x_{HR}[b_{c,1} \ratio b_{c,2},\, b_{c,3} \ratio b_{c,4}], \quad x_c=\zeta(x_{c, HR},\,1/s)
$$

Crop bounding box $b_c$는 이후에 합치는 과정에서 정확히 정렬하기 위해서 $o \ge 1$인 $k=s\sdot o$에 의해 정확히 나누어 떨어지도록 이미지 크기 내에서 discrete uniform distribution을 통해 sampling됩니다.

$$
b_{c,1} \sim \{0, (H-sh_c)/k \} \sdot k, \quad b_{c,2} = b_{c,1} + sh_c, \\
b_{c,3} \sim \{0, (W-sw_c)/k \} \sdot k, \quad b_{c,4} = b_{c,3} + sw_c, 
$$

Detail crop $x_d \in \mathbb R^{h_d \times w_d \times 3}$은 context와 detail에서의 예측을 나중에 합치기 위해서 context crop region안에서 randonm하게 crop됩니다.

$$
x_d = x_{x_c, HR}[b_{d, 1} \ratio b_{d, 2},\, b_{d, 3} \ratio b_{d, 4}], \\
b_{d,1} \sim \{0, (sh_c -h_d)\} \sdot k, \quad b_{d, 2} = b_{d,1} + h_d,\\
b_{d,3} \sim \{0, (sw_c - w_d)\} \sdot k, \quad b_{d, 4} = b_{d, 3} + w_d
$$

저자들은 동일한 차원의 context와 detail crop을 사용했습니다. 즉, $h_c = h_d$이고 $w_c=w_d$입니다. LR을 사용한 이전 UDA 방법에 따라 downscale scale factor는 $s = 2$로 설정했습니다. 이는 context crop은 detail crop에 비해 4배 많은 내용을 담는 것을 의미합니다.

Feature encoder $f^E$와 semantic decoder $f^S$를 사용하여, context semantic segmentation $\hat y_c=f^S(f^E(x_c)) \in \mathbb R ^{\frac{h_c}{o} \times \frac{w_c}{o}\times C}$과 detail semantic segmentation $\hat y_d=f^S(f^E(x_d)) \in \mathbb R^{\frac{h_d}{o} \times \frac{w_d}{o}\times C}$을 예측합니다. HR과 LR입력에 대해 같은 network $f_E$와 $f_S$를 사용합니다. 이것은 메모리 사용량을 줄일뿐만 아니라 서로 다른 해상도에 대한 network의 robustness를 증가시킵니다.

## Multi-Resolution Fusion
HR detail crop은 pole이나 distant pedestrian과 같이 작은 객체를 segmentation하는데 매우 적합합니다. 반면에, long-range dependecy를 잡아내는 능력은 부족합니다. LR context crop은 이와 반대입니다. 그러므로, 저자들은 context와 detail crop에서의 예측을 신뢰할 이미지 영역을 예측하기 위한 학습가능한 scale attention을 사용하여 두 crop을 통한 예측을 합칩니다. 또한, scale attention은 객체를 좀 더 적합한 scale에서 adpat할 수 있는 이점을 제공합니다. 예를 들어, 작은 객체는 HR에서 adapt하기 쉬운 반면, 큰 객체는 LR에서 adapt하기 쉽습니다. 왜냐하면, 객체의 외관이 해상도가 너무 높은면 network가 domain-specific detailed texture에 과적합될 수 있기 때문입니다. 

Scale attention decoder $f_A$는 LR context와 HR detail prediction이 어느 정도 비중을 두고 신뢰할 수 있는지 판단하기 위해 scale attention $a_c=\sigma(f_A(f_E(x_c))) \in [0, 1]^{\frac{h_c}{o} \times \frac{w_c}{o} \times C}$을 학습합니다. Sigmoid 함수 $\sigma$는 weight가 $[0, 1]$안에 있도록 보장합니다. 여기서, 1은 HR detail crop에 초점을 맞추는 것을 의미합니다. Attention은 context crop으로부터 예측됩니다. 왜냐하면, context crop이 scene layout(larger context)를 더 잘 파악할 수 있기 때문입니다. Output stride $o$로 인해 입력이 예측보다 작기 때문에서, 다음 단계에서 crop 좌표의 크기는 조정됩니다. Detail crop $c_d$의 바깥의 detail prediction이 없기 때문에 attention은 0으로 설정됩니다.

$$
a_c^\prime \in \mathbb R^{\frac{h_c}{o}\times\frac{w_c}{o}}, \quad a_c^\prime=\begin{cases} a_c(i,j) &\text{if} \space \frac{b_{d,1}}{s\sdot o} \le i \lt \frac{b_{d,2}}{s \sdot o} \land \frac{b_{d,3}}{s \sdot o} \le j \lt \frac{b_{d,4}}{s \sdot o} \\ 0 &\text{otherwise} \end{cases}
$$

Detail crop은 0으로 padding하여 $\frac{sh_c}{o} \times \frac{sw_c}{o}$의 크기로(upsampled) context crop과 일치하게 정렬됩니다.

$$
\hat y_d^\prime(i,j)=\begin{cases} \hat y_d(i-\frac{b_{d,1}}{o},j-\frac{b_{d,3}}{o}) &\text{if} \space \frac{b_{d,1}}{o} \le i \lt \frac{b_{d,2}}{o} \land \frac{b_{d,3}}{o} \le j \lt \frac{b_{d,4}}{o}

\\ 0 &\text{otherwise} \end{cases}
$$

최종적으로 다양한 scale의 예측은 attention-weighted sum을 통해 합쳐집니다.

$$
\hat y_{c,F} =\zeta((1-a_c^\prime) \odot \hat y_c, s) + \zeta(a_c^\prime, s) \sdot \hat y_{d}^\prime
$$

Encoder, segmentation head 그리고 attention head는 합쳐진 multi-scale 예측과 detail crop 예측을 통해 훈련됩니다.

$$
\mathcal L_{HRDA}^S=(1-\lambda)\mathcal L_{ce}(\hat y_{c,F}^{S},y_c^S,1) + \lambda_d\mathcal L_{ce}(\hat y_d^S,y_d^S,1)
$$

Detail crop에 대한 항이 존재하는 이유는 비록 attention이 해당 영역에서 context crop에 더 가중을 두더라도 HR 입력에 대해 더 robust한 feature를 학습하는데 도움이 되기 때문입니다. Target loss $\mathcal L_{HRDA}^T$는 아래와 같습니다.

$$
\mathcal L_{HRDA}^T = (1-\lambda_d)\mathcal L_{ce}(\hat y_{c,F}^T, p_{c,F}^T, q_{c,F}^T) + \lambda_d \mathcal L_{ce}(\hat y_d^T, p_d^T, q_d^T)
$$

pseudo-label의 생성에도 multi-resolution fusion을 사용하였습니다. 이로 인해, pseudo-label을 예측할 때도 scale attention을 통해 더 적합한 해상도에 초점을 맞춥니다. Pseudo-label은 덜 적합한 해상도(예: 작은 객체에 대한 LR)로 모델을 훈련시키는데에도 사용되며 이는 작은 객체와 큰 객체 모두에 대한 robustness가 향상된다고 합니다. 필자가 생각하기로는 HRDA prediction $\hat y_{c,F}^T$중 HR이 합쳐지지 않은 영역은 context prediciton이며 이 부분은 작은 객체에 대해 덜 적합한 해상도라 생각됩니다. 따라서 이 부분에 대한 loss가 덜 적합한 해상도로 모델을 훈련시키는데 사용되었다고 볼 수 있을 것 같습니다.

## Pseudo-Label Generation with Overlapping Sliding Window

Self-training의 경우, context crop $x_{c,HR}^T$를 위한 고품질의 HRDA pseudo-label $p_{c,F}^T$을 생성할 필요가 있습니다. LR prediction $\hat y_c^T$과 HR prediction $\hat y_{c,HR}^T$을 full scale attention $a_c^T$를 통해 합쳐서 HRDA prediction $\hat y_{c,F}^T$을 생성합니다. 

$$
\hat y_{c,F}^T=\zeta((1-a_c^T) \odot \hat y_c^T,s)+\zeta(a_c^T,s) \odot \hat y_{c,HR}^T
$$

비록 큰 HR network 입력은 훈련중에는 문제가 될 수 있지만, pseudo-label을 추론할 때는 역전파 과정이 없기 때문에 큰 문제가 되지 않습니다. 하지만, DAFormer 혹은 다른 Vision Transformer도 마찬가지로 훈련과 추론의 입력 크기가 동일할 때 (implicit) positional embedding이 잘 작동합니다. 그러므로, $h_d \times w_d$크기의 sliding window를 HR context crop $x_{c,HR}^T$에 적용하여 HR prediction $\hat y_{c,HR}^T$를 추론합니다. 여기서, window는 서로 다른 context을 통해 overlapping prediction을 생성하기 위해 $h_d/2 \times w_d/2$의 stride로 shift됩니다. Sliding window를 통한 crop images의 생성은 batch를 통해 병렬 처리 할 수 있습니다. 즉, GPU에서 매우 효율적으로 계산할 수 있습니다.

Model validation이나 deployment의 경우, 전체 이미지 $x_{HR}$에 대한 full-scale HRDA semantic segmentation $\hat y_{F,HR}$가 필요합니다. Context crop은 일반적으로 전체 이미지보다 작기 때문에, $\hat y_{F,HR}$은  전체 이미지 $x_{HR}$에 대해 $sh_c \times sw_c$의 크기와 $sh_c/2 \times sw_c/2$의 stride으로 overlapping sliding window를 통해 생성합니다.

# Conclusion

이 연구에서는 감당할 수 있는 GPU 메모리 공간을 유지하면서 학습 가능한 scale attention을 통하여 작은 HR detail crop과 큰 LR context crop의 이점을 결합한 UDA를 위한 multi-resolution 접근인 HRDA를 제안했습니다. 이것은 다양한 UDA 방법과 결합할 수 있고 일관성있게 상당한 성능 향상을 달성했습니다. 전체적으로, HRDA는 GTA5 $\rightarrow$ Cityscapes에서 73.8 mIoU와 SYNTHIA $\rightarrow$ Cityscapes에서 65.8 mIoU의 전례 없는(unprecedented) 성능을 각각 달성하였습니다. 이는 이전 SOTA에 비해 각각 +5.5 mIoU와 +4.9 mIoU가 올라간 수치입니다.
